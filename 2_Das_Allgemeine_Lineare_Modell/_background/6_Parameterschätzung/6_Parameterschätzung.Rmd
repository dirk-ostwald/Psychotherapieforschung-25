---
fontsize: 8pt
bibliography: 6_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 6_header.tex
---

```{r, include = F}
source("6_R_common.R")
fdir        = file.path(getwd(), "6_Abbildungen")                                
```

#  {.plain}

\center

```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("6_Abbildungen/alm_6_otto.png")
```

\vspace{2mm}

\huge

Allgemeines Lineares Modell \vspace{6mm}

\large

BSc Psychologie SoSe 2023

```{=tex}
\vspace{6mm}
\normalsize
```
Prof. Dr. Dirk Ostwald

#  {.plain}

```{=tex}
\center
\huge
\vfill
```
\noindent (6) Parameterschätzung
\vfill

#
\large

Naturwissenschaft \vspace{7mm}

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("6_Abbildungen/alm_6_wissenschaft.pdf")
```

#
\vspace{1mm}
\normalsize
Modellformulierung
\vspace{1mm}
\small
\begin{equation}
\upsilon = X\beta + \varepsilon, \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
\vspace{5mm}

\normalsize
Modellschätzung
\small
\begin{equation}
\hat{\beta} = (X^TX)^{-1} X^T\upsilon,  \hat{\sigma}^2 = \frac{(\upsilon-X\hat{\beta})^T(\upsilon-X\hat{\beta})}{n-p}
\end{equation}
\vspace{4mm}

\normalsize
Modellevaluation
\small
\begin{equation}
T = \frac{c^T\hat{\beta} - c^T\beta_0}{\sqrt{\hat{\sigma}^2c^T(X^TX)^{-1}c}},
F = \frac{(\hat{\varepsilon}_0^T\hat{\varepsilon}_0 - \hat{\varepsilon}^T\hat{\varepsilon})/p_1}{\hat{\varepsilon}^T\hat{\varepsilon}/(n-p)}
\end{equation}

#
Standardprobleme Frequentistischer Inferenz

\small
\vspace{2mm}
\noindent (1) Parameterschätzung

Ziel der Parameterschätzung ist es, einen möglichst guten Tipp für wahre, aber unbekannte,
Parameterwerte oder Funktionen dieser abzugeben, typischerweise mithilfe von Daten.

\vspace{2mm}
\noindent (2) Konfidenzintervalle

Ziel der Bestimmung von Konfidenzintervallen ist es, basierend auf der angenommenen
Verteilung der Daten eine quantitative Aussage über die mit Schätzwerten assoziierte
Unsicherheit zu treffen.

\vspace{2mm}
\noindent (3) Hypothesentests

Ziel des Hypothesentestens ist es, basierend auf der angenommenen
Verteilung der Daten in einer möglichst zuverlässigen Form zu entscheiden, ob ein
wahrer, aber unbekannter Parameterwert in einer von zwei sich gegenseitig
ausschließenden Untermengen des Parameterraumes liegt.

#
\center
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("6_Abbildungen/alm_6_frequentistische_inferenz.pdf")
```
\center
\footnotesize
$\theta := (\beta,\sigma^2)$,
$\Theta := \mathbb{R}^p \times \mathbb{R}_{>0}$
$\mathbb{P}_\theta(\upsilon) := \mathbb{P}_{\beta,\sigma^2}(\upsilon)$ mit WDF $p_{\beta,\sigma^2}(y) := N(y;X\beta,\sigma^2I_n)$

#
\small
Standardannahmen Frequentistischer Inferenz

\footnotesize
\setstretch{1.2}
Gegeben sei das Allgemeine Lineare Modell. Es wird angenommen, dass ein
vorliegender Datensatz eine der möglichen Realisierungen der Daten des Modells ist.
Aus Frequentistischer Sicht kann man unendlich oft Datensätze basierend auf einem
Modell generieren und zu jedem Datensatz Schätzer oder Statistiken auswerten, z.B.
den Betaparameterschätzer
\vspace{1mm}

\begin{itemize}
\item[] Datensatz (1) : $y^{(1)} = \left(y_1^{(1)}, y_2^{(1)}, ...,y_n^{(1)}\right)^T$ 	mit $\hat{\beta}^{(1)} = (X^TX)^{-1}X^Ty^{(1)}$
\item[] Datensatz (2) : $y^{(2)} = \left(y_1^{(2)}, y_2^{(2)}, ...,y_n^{(2)}\right)^T$ 	mit $\hat{\beta}^{(2)} = (X^TX)^{-1}X^Ty^{(2)}$
\item[] Datensatz (3) : $y^{(3)} = \left(y_1^{(3)}, y_2^{(3)}, ...,y_n^{(3)}\right)^T$ 	mit $\hat{\beta}^{(3)} = (X^TX)^{-1}X^Ty^{(3)}$
\item[] Datensatz (4) : $y^{(4)} = \left(y_1^{(4)}, y_2^{(4)}, ...,y_n^{(4)}\right)^T$ 	mit $\hat{\beta}^{(4)} = (X^TX)^{-1}X^Ty^{(4)}$
\item[] Datensatz (5) : $y^{(5)} = ...$
\end{itemize}

\vspace{1mm}
Um die Qualität statistischer Methoden zu beurteilen betrachtet die Frequentistische
Statistik die Wahrscheinlichkeitsverteilungen von Schätzern und Statistiken
unter Annahme der Datenverteilung. Was zum Beispiel ist die Verteilung von
$\hat{\beta}^{(1)}$, $\hat{\beta}^{(2)}$, $\hat{\beta}^{(3)}$, $\hat{\beta}^{(4)}$, ... also die
Verteilung der Zufallsvariable $\hat{\beta} := (X^TX)^{-1}X^T\upsilon$? Wenn eine statistische
Methode im Sinne der Frequentistischen Standardannahmen "gut" ist, dann heißt das
also, dass sie bei häufiger Anwendung "im Mittel gut" ist. Im Einzelfall, also
im Normalfall nur eines vorliegenden Datensatzes, kann sie auch "schlecht" sein.


#
\large
\setstretch{2.7}
\vfill
Allgemeine Theorie

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

Frequentistische Schätzerverteilungen

Selbstkontrollfragen
\vfill

#
\large
\setstretch{2.7}
\vfill
**Allgemeine Theorie**

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

Frequentistische Schätzerverteilungen

Selbstkontrollfragen
\vfill

# Allgemeine Theorie
\footnotesize
\begin{theorem}[Betaparameterschätzer]
\justifying
\normalfont
Es sei
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
das ALM und es sei
\begin{equation}
\hat{\beta} := \left(X^TX\right)^{-1}X^T\upsilon.
\end{equation}
der \textit{Betaparameterschätzer}. Dann gilt, dass $\hat{\beta}$ die Summe der Abweichungsquadrate minimiert,
\begin{equation}
\hat{\beta} = \argmin_{\tilde{\beta}} (\upsilon-X\tilde{\beta})^T(\upsilon-X\tilde{\beta}),
\end{equation}
und dass $\hat{\beta}$ ein unverzerrter Maximum-Likelihood Schätzer von $\beta \in \mathbb{R}^p$ ist.
\end{theorem}

Bemerkungen

* Das Theorem gibt ein Formel an, um $\beta$ anhand von Designmatrix und Daten zu schätzen.
* Da $\hat{\beta}$ die Summe der Abweichungsquadrate minimiert, heißt $\hat{\beta}$ auch Kleinste-Quadrate (KQ) Schätzer.
* Die $\tilde{\beta}$ Notation des Maximierungarguments dient lediglich zur Abgrenzung vom w.a.u. $\beta$.
* Als ML Schätzer ist $\hat{\beta}$ weiterhin konsistent, asymptotisch normalverteilt und asymptotisch effizient.
* Wir sehen später, dass $\hat{\beta}$ sogar normalverteilt ist.
* Außerdem hat $\hat{\beta}$ die "kleinste Varianz" in der Klasse der linearen unverzerrten Schätzer von $\beta$.
* Letztere Eigenschaft ist Kernaussage des \textit{Gauss-Markov Theorems}, auf das wir hier nicht näher eingehen wollen.
* Für eine Diskussion und einen Beweis des Gauss-Markov Theorems siehe z.B. @searle_1971, Kapitel 3.

# Allgemeine Theorie

\footnotesize
\underline{Beweis}

\noindent (1) Wir zeigen in einem ersten Schritt, dass $\hat{\beta}$ die Summe der Abweichungsquadrate
\begin{equation}
(\upsilon-X\tilde{\beta})^T(\upsilon-X\tilde{\beta})
\end{equation}
minimiert. Dazu halten wir zunächst fest, dass
\begin{equation}
\hat{\beta} = (X^TX)^{-1}X^T\upsilon
\Leftrightarrow
X^TX\hat{\beta} = X^T\upsilon
\Leftrightarrow
X^T\upsilon - X^TX\hat{\beta} = 0_p
\Leftrightarrow
X^T(\upsilon -   X\hat{\beta}) = 0_p.
\end{equation}
Weiterhin gilt dann auch, dass
\begin{equation}
X^T(\upsilon -  X\hat{\beta}) = 0_p
\Leftrightarrow
\left(X^T(\upsilon -  X\hat{\beta})\right)^T = 0_p^T
\Leftrightarrow
(\upsilon -  X\hat{\beta})^TX = 0_p^T
\end{equation}
Weiterhin halten wir ohne Beweis fest, dass für jede Matrix $X \in \mathbb{R}^{n \times p}$ gilt, dass
\begin{equation}
z^TX^TXz \ge 0 \mbox{ für alle } z \in \mathbb{R}^p.
\end{equation}
Wir betrachten nun die Summe der Abweichungsquadrate
\begin{equation}
(\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta}).
\end{equation}

# Allgemeine Theorie

\footnotesize
\underline{Beweis (fortgeführt)}

Es ergibt sich dann
\begin{align*}
\begin{split}
& (\upsilon-X\tilde{\beta})^T(\upsilon- X\tilde{\beta}) \\
& = (\upsilon-X\hat{\beta} + X\hat{\beta}- X\tilde{\beta})^T(\upsilon-X\hat{\beta} + X\hat{\beta} - X\tilde{\beta}) \\
& = ((\upsilon- X\hat{\beta}) + X(\hat{\beta}-\tilde{\beta}))^T((\upsilon-X\hat{\beta}) + X(\hat{\beta} -\tilde{\beta})) \\
& = (\upsilon-X\hat{\beta})^T(\upsilon- X\hat{\beta}) + (\upsilon -  X\hat{\beta})^T X(\hat{\beta} -\tilde{\beta})
     + (\hat{\beta}-\tilde{\beta})^TX^T(\upsilon- X\hat{\beta})
     + (\hat{\beta}-\tilde{\beta})^TX^TX(\hat{\beta} -\tilde{\beta}) \\
& = (\upsilon -  X\hat{\beta})^T(\upsilon -  X\hat{\beta})  +  0_p^T(\hat{\beta} -\tilde{\beta})
     + (\hat{\beta} -\tilde{\beta})^T0_p
     + (\hat{\beta} -\tilde{\beta})^TX^TX(\hat{\beta} -\tilde{\beta}) \\
&  = (\upsilon- X\hat{\beta})^T(\upsilon-X\hat{\beta}) + (\hat{\beta} -\tilde{\beta})^TX^TX(\hat{\beta} -\tilde{\beta}). \\
\end{split}
\end{align*}
Auf der rechten Seite obiger Gleichung ist nur der zweite Term von $\tilde{\beta}$ abhängig. Da für diesen Term gilt, dass
\begin{equation}
(\hat{\beta} -\tilde{\beta})^TX^TX(\hat{\beta} -\tilde{\beta}) \ge 0
\end{equation}
nimmt dieser Term genau dann seinen Minimalwert 0 an, wenn
\begin{equation}
(\hat{\beta} -\tilde{\beta}) = 0_p \Leftrightarrow \tilde{\beta} = \hat{\beta}.
\end{equation}
Also gilt
\begin{equation}
\hat{\beta} = \argmin_{\tilde{\beta}} (\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta}).
\end{equation}

# Allgemeine Theorie
\footnotesize
\underline{Beweis (fortgeführt)}

\noindent (2) Um zu zeigen, dass $\hat{\beta}$ ein Maximum Likelihood Schätzer ist,
betrachten wir für festes $y \in \mathbb{R}^n$ und festes $\sigma^2 > 0$ die
Log-Likelihood Funktion
\begin{equation}
\ell : \mathbb{R}^p \to \mathbb{R}, \tilde{\beta} \mapsto \ln p_{\tilde{\beta}}(y) = \ln N(y;X\tilde{\beta}, \sigma^2I_n)
\end{equation} wobei gilt, dass
\begin{align}
\begin{split}
\ln N(y;X\tilde{\beta}, \sigma^2I_n)
& = \ln\left((2\pi)^{-\frac{n}{2}}|\sigma^2I_n|^{-\frac{1}{2}}\exp\left(-\frac{1}{2\sigma^2}(\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta})\right)\right) \\
& = -\frac{n}{2} \ln 2\pi - \frac{1}{2} \ln |\sigma^2I_n| - \frac{1}{2\sigma^2}(\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta}).
\end{split}
\end{align}
Dabei hängt allein der Term $-\frac{1}{2\sigma^2}(\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta})$
von $\tilde{\beta}$ ab. Weil aber $(\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta}) \ge 0$,
gilt wird dieser Term aufgrund des negativen Vorzeichen maximal, wenn
$(\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta})$ minimal wird. Dies ist aber wie oben
gezeigt genau für $\tilde{\beta} = \hat{\beta}$ der Fall.

\vspace{2mm}
\noindent (3) Die Unverzerrtheit von $\hat{\beta}$ schließlich ergibt sich aus \begin{align}
\begin{split}
\mathbb{E}(\hat{\beta})
= \mathbb{E}\left((X^TX)^{-1}X^T\upsilon\right)
= (X^TX)^{-1}X^T\mathbb{E}(\upsilon)
= (X^TX)^{-1}X^TX\beta
= \beta.
\end{split}
\end{align}


# Allgemeine Theorie
\footnotesize
\begin{definition}[Erklärte Daten, Residuenvektor, Residuen]
Es sei 
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n) 
\end{equation}
das Allgemeine Lineare Modell und es sei 
\begin{equation}
\hat{\beta} := (X^TX)^{-1}X^T\upsilon. 
\end{equation}
der Betaparameterschätzer. Dann heißt der Zufallsvektor
\begin{equation}
\hat{\upsilon} := X\hat{\beta} = X(X^TX)^{-1}X^T\upsilon
\end{equation}
die \textit{erklärten Daten}, der Zufallsvektor 
\begin{equation}
\hat{\varepsilon} := \upsilon - \hat{\upsilon} = \upsilon - X\hat{\beta}
\end{equation}
heißt \textit{Residuenvektor} und für $i = 1,...,n$ heißen die Komponenten dieses Zufallsvektors  
\begin{equation}
\hat{\varepsilon}_i := \upsilon_i - \hat{\upsilon}_i =  \upsilon_i - (X\hat{\beta})_i 
\end{equation}
die \textit{Residuen}. 
\end{definition}

Bemerkungen

* Die Begriffe sind analog zu den in Einheit (2) Korrelation eingeführten Begriffen.



# Allgemeine Theorie
\footnotesize
\setstretch{1.3}
\begin{theorem}[Varianzparameterschätzer]
\justifying
\normalfont
Es sei
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
das ALM in generativer Form. Dann ist
\begin{equation}
\hat{\sigma}^2 := \frac{(\upsilon-X\hat{\beta})^T(\upsilon-X\hat{\beta})}{n - p}
\end{equation}
ein unverzerrter Schätzer von $\sigma^2 > 0$.
\end{theorem}
\vspace{-2mm}

\footnotesize

Bemerkungen

* Es handelt sich bei $\hat{\sigma}^2$ \textit{nicht} um einen Maximum Likelihood Schätzer von $\sigma^2$.
* Für einen Beweis siehe z.B. @searle_1971, Kapitel 3 oder @rencher_2008, Kapitel 7.
* Mit Definition des Residuenvektors und der Residuen bieten sich für $\hat{\sigma}^2$ auch folgende Schreibweisen an:
\begin{equation}
\hat{\sigma}^2
= \frac{\hat{\varepsilon}^T \hat{\varepsilon}}{n-p}
= \frac{1}{n-p} \sum_{i=1}^n \hat{\varepsilon}_i^2
= \frac{1}{n-p} \sum_{i=1}^n \left(\upsilon_i - (X\beta)_i \right)^2
\end{equation}
* $\sigma^2$ wird also durch die eine skalierte Residualquadratsumme geschätzt.
* Der Maximum Likelihood Schätzer des Varianzparameters ist $\hat{\sigma}^2_{\tiny \mbox{ML}} := \frac{1}{n}\hat{\varepsilon}^T \hat{\varepsilon}$.

#
\large
\setstretch{2.7}
\vfill
**Allgemeine Theorie**

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

Frequentistische Schätzerverteilungen

Selbstkontrollfragen
\vfill


# Unabhängige und identisch normalverteilte Zufallsvariablen
\small

Wir betrachten das Szenario von $n$ unabhängigen und identisch normalverteilten
Zufallsvariablen mit Erwartungswertparameter $\mu \in \mathbb{R}$ und
Varianzparameter $\sigma^2$,
\begin{equation}
\upsilon_i \sim N(\mu,\sigma^2) \mbox{ für } i = 1,...,n.
\end{equation}
Dann gilt, wie unten gezeigt,
\begin{equation}\label{eq:iid_estimators}
\hat{\beta} = \frac{1}{n}\sum_{i=1}^n \upsilon_i =: \bar{\upsilon}
\mbox{ und }
\hat{\sigma}^2 = \frac{1}{n-1}\sum_{i=1}^n (\upsilon_i - \bar{\upsilon})^2 =: s^2_\upsilon.
\end{equation}
In diesem Fall ist also der Betaparameterschätzer mit dem Stichprobenmittel
$\bar{\upsilon}$ der $\upsilon_1,...,\upsilon_n$ und der Varianzparameterschätzer mit der
Stichprobenvarianz $s_\upsilon^2$ der $\upsilon_1,...,\upsilon_n$ identisch.

# Unabhängige und identisch normalverteilte Zufallsvariablen
\small
Für $\hat{\beta}$ ergibt sich
\begin{align*}
\begin{split}
\hat{\beta}
& = (X^TX)^{-1}X^T\upsilon
\\
& = \left(1_n^T 1_n\right)^{-1}1_n^T\upsilon
\\
& =
\left(
\begin{pmatrix}
1 & \cdots & 1\\
\end{pmatrix}
\begin{pmatrix}
1\\
\vdots \\
1\\
\end{pmatrix}
\right)^{-1}
\begin{pmatrix}
1 & \cdots & 1\\
\end{pmatrix}
\begin{pmatrix}
\upsilon_{1}\\
\vdots \\
\upsilon_{n}\\
\end{pmatrix}
\\
& = n^{-1}\sum_{i=1}^n \upsilon_i
\\
& =\frac{1}{n}\sum_{i=1}^n \upsilon_i
\\
& =: \bar{\upsilon}.
\end{split}
\end{align*}

# Unabhängige und identisch normalverteilte Zufallsvariablen
\small
Für $\hat{\sigma}^2$ ergibt sich
\begin{align*}
\begin{split}
\hat{\sigma}^2
& = \frac{1}{n-1}\left(\upsilon-X\hat{\beta}\right)^T\left(\upsilon-X\hat{\beta} \right)
\\
& = \frac{1}{n-1}\left(\upsilon-1_n\bar{\upsilon}\right)^T\left(\upsilon-1_n\bar{\upsilon}\right)
\\
& = \frac{1}{n-1}
\left(
\begin{pmatrix} \upsilon_1  \\  \vdots  \\  \upsilon_n  \end{pmatrix} -
\begin{pmatrix} 1           \\  \vdots  \\  1           \end{pmatrix}
\bar{\upsilon}
\right)^T
\left(
\begin{pmatrix} \upsilon_1  \\  \vdots  \\  \upsilon_n  \end{pmatrix} -
\begin{pmatrix} 1           \\  \vdots  \\  1           \end{pmatrix}
\bar{\upsilon}
\right)
\\
& =
\frac{1}{n-1}
\begin{pmatrix}
\upsilon_{1}-\bar{\upsilon}
& \cdots
& \upsilon_{n}-\bar{\upsilon}
\end{pmatrix}
\begin{pmatrix}
\upsilon_{1}-\bar{\upsilon} \\
\vdots \\
\upsilon_{n}-\bar{\upsilon} \\
\end{pmatrix}
\\
& = \frac{1}{n-1} \sum_{i=1}^n \left(\upsilon_i-\bar{\upsilon} \right)^2
\\
& =: s^2_\upsilon.
\end{split}
\end{align*}

# Unabhängige und identisch normalverteilte Zufallsvariablen
Parameterschätzung
\vspace{1mm}

\setstretch{1}
\tiny
```{r}
# Modellformulierung
library(MASS)                                     # Multivariate Normalverteilung
n          = 12                                   # Anzahl Datenpunkte
p          = 1                                    # Anzahl Betaparameter
X          = matrix(rep(1,n), nrow = n)           # Designmatrix
I_n        = diag(n)                              # n x n Einheitsmatrix
beta       = 2                                    # wahrer, aber unbekannter, Betaparameter
sigsqr     = 1                                    # wahrer, aber unbekannter, Varianzparameter

# Datenrealisierung
y          =  mvrnorm(1, X %*% beta, sigsqr*I_n)  # eine Realisierung eines n-dimensionalen ZVs

# Parameterschätzung
beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y     # Betaparameterschätzer
eps_hat    = y - X %*% beta_hat                   # Residuenvektor
sigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)      # Varianzparameterschätzer

# Ausgabe
cat("beta       : ", beta,
    "\nhat{beta}  : ", beta_hat,
    "\nsigsqr     : ", sigsqr,
    "\nhat{sigsqr}: ", sigsqr_hat)
```

# Unabhängige und identisch normalverteilte Zufallsvariablen
Simulation der Schätzerunverzerrtheit
\vspace{1mm}

\setstretch{1}
\tiny
```{r}
# Modellformulierung
library(MASS)                                        # Multivariate Normalverteilung
n         = 12                                       # Anzahl Datenpunkte
p          = 1                                       # Anzahl Betaparameter
X          = matrix(rep(1,n), nrow = n)              # Designmatrix
I_n        = diag(n)                                 # n x n Einheitsmatrix
beta       = 2                                       # wahrer, aber unbekannter, Betaparameter
sigsqr     = 1                                       # wahrer, aber unbekannter, Varianzparameter

# Frequentistische Simulation
nsim       = 1e4                                     # Anzahl Datenrealisierungen
beta_hat   = rep(NaN,nsim)                           # \hat{\beta} Realisierungsarray
sigsqr_hat = rep(NaN,nsim)                           # \hat{sigsqr} Realisierungsarray
for(i in 1:nsim){                                    # Simulationsiterationen
  y             = mvrnorm(1, X %*% beta, sigsqr*I_n) # Datenrealisierung
  beta_hat[i]   = solve(t(X) %*% X) %*% t(X) %*% y   # Betaparameterschätzer
  eps_hat       = y - X %*% beta_hat[i]              # Residuenvektor
  sigsqr_hat[i] = (t(eps_hat) %*% eps_hat) /(n-p)    # Varianzparameterschätzer
}

# Ausgabe
cat("Wahrer, aber unbekannter, Betaparameter                  : ", beta,
    "\nGeschätzter Erwartungswert des Betaparameterschätzers    : ", mean(beta_hat),
    "\nWahrer, aber unbekannter, Varianzparameter               : ", sigsqr,
    "\nGeschätzter Erwartungswert des Varianzparameterschätzers : ", mean(sigsqr_hat))
```

#
\large
\setstretch{2.7}
\vfill
Allgemeine Theorie

Unabhängige und identisch normalverteilte Zufallsvariablen

**Einfache lineare Regression**

Frequentistische Schätzerverteilungen

Selbstkontrollfragen
\vfill


# Einfache lineare Regression
\small

Wir betrachten das  Modell der einfachen linearen Regression 
\begin{equation}\label{eq:slr}
\upsilon_i = \beta_0 + \beta_1x_i + \varepsilon_i, \varepsilon_i \sim N(0,\sigma^2) \mbox{ für } i = 1,...,n,
\end{equation} Dann gilt wie unten gezeigt, dass
\begin{equation}\label{eq:slr_estimators}
\hat{\beta}
= \begin{pmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{pmatrix}
= \begin{pmatrix} \bar{\upsilon} - \frac{c_{x\upsilon}}{s_x^2}\bar{x} \\ \frac{c_{x\upsilon}}{s_x^2} \end{pmatrix}
\mbox{ und }
\hat{\sigma}^2 = \frac{1}{n-2}\sum_{i=1}^n (\upsilon_i - (\hat{\beta}_0 + \hat{\beta}_1x_i))^2
\end{equation} 
wobei 

* $\bar{x}$ und $\bar{\upsilon}$ die Stichprobenmittel der $x_1,...,x_n$ und $\upsilon_1,...,\upsilon_n$, respektive, bezeichnen 
* $c_{x\upsilon}$ die Stichprobenkovarianz der $x_1, ...,x_n$ und $\upsilon_1,...,\upsilon_n$ bezeichnet
* $s_x^2$ die Stichprobenvarianz der $x_1,...,x_n$ bezeichnet.

Wie in (1) Regression sind die Bezeichnungen "Stichproben"kovarianz und "Stichproben"varianz bezüglich
der $x_1,...,x_n$ hier lediglich formal gemeint, da keine Annahme zugrundeliegt,
dass die $x_1,...,x_n$ Realisierungen von Zufallsvariablen sind.
Die $x_1,...,x_n$ sind vorgegebene Werte. 


# Einfache lineare Regression
\small
Wir halten fest, dass für eine parametrische Designmatrixspalte sich der entsprechende
Betaparameterschätzer aus der Stichprobenkovarianz der respektiven Spalte mit den Daten
geteilt durch die Stichprobenvarianz der entsprechenden Spalte ergibt und somit
einer "standardisierten" Stichprobenkovarianz entspricht. 
\vspace{2mm}

Ein Vergleich mit den Parametern der Ausgleichsgerade in (1) Regression zeigt weiterhin die Identität der
Betaparameterschätzerkomponenten $\hat{\beta}_0$ und $\hat{\beta}_1$ mit den
dort unter dem Kriterium der Minimierung der quadrierten vertikalen Abweichungen
hergeleiteten Parametern. Dies überrascht nicht, da sowohl $\hat{\beta}$ als auch
die Parameter der Ausgleichsgerade den Wert
\begin{equation}
q(\tilde{\beta})
= \sum_{i=1}^n (\upsilon_i - (\tilde{\beta}_0 + \tilde{\beta}_1 x_i))^2
= (\upsilon -  X\tilde{\beta})^T(\upsilon -  X\tilde{\beta})
\end{equation}
hinsichtlich $\tilde{\beta}$ minimieren.

# Einfache lineare Regression
\footnotesize

Um die Form des Betaparameterschätzers herzuleiten, halten wir zunächst fest, dass 
\tiny \begin{align}
\begin{split}
\sum_{i=1}^n (x_i-\bar{x})(\upsilon_i-\bar{\upsilon})
& = \sum_{i=1}^n (x_i\upsilon_i - x_i\bar{\upsilon} - \bar{x}\upsilon_i + \bar{x}\bar{\upsilon}) \\
& = \sum_{i=1}^n x_i\upsilon_i
  - \sum_{i=1}^n x_i\bar{\upsilon}
  - \sum_{i=1}^n \bar{x}\upsilon_i
  + \sum_{i=1}^n \bar{x}\bar{\upsilon} \\
& = \sum_{i=1}^n x_i\upsilon_i
  - \bar{\upsilon} \sum_{i=1}^n x_i
  - \bar{x}\sum_{i=1}^n \upsilon_i
  + n \bar{x}\bar{\upsilon} \\
& = \sum_{i=1}^n x_i\upsilon_i
  - \bar{\upsilon}n\bar{x}
  - \bar{x}n\bar{\upsilon}
  + n\bar{x}\bar{\upsilon} \\
  & = \sum_{i=1}^n x_i\upsilon_i
  - n\bar{x}\bar{\upsilon}
  - n\bar{x}\bar{\upsilon}
  + n\bar{x}\bar{\upsilon} \\
& = \sum_{i=1}^n x_i \upsilon_i  - n \bar{x}\bar{\upsilon},
\end{split}
\end{align}

# Einfache lineare Regression
\footnotesize

Weiterhin halten wir fest, dass
\tiny
\begin{align}
\begin{split}
\sum_{i=1}^n (x_i - \bar{x})^2
& = \sum_{i=1}^n (x_i^2 - 2x_i\bar{x}  + \bar{x}^2) \\
& = \sum_{i=1}^n x_i^2 - \sum_{i=1}^n 2x_i\bar{x}  + \sum_{i=1}^n \bar{x}^2 \\
& = \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i  + n\bar{x}^2 \\
& = \sum_{i=1}^n x_i^2 - 2\bar{x}n\bar{x}  + n\bar{x}^2 \\
& = \sum_{i=1}^n x_i^2 - 2n\bar{x}^2 + n\bar{x}^2 \\
& = \sum_{i=1}^n x_i^2 - n\bar{x}^2. 
\end{split}
\end{align}

# Einfache lineare Regression
\footnotesize

Aus der Definition von $\hat{\beta}$ ergibt sich \begin{align}
\begin{split}
\hat{\beta}
& = (X^T X)^{-1}X^T\upsilon \\
& =
\left(
\begin{pmatrix}
1   & \cdots & 1 \\
x_1 & \cdots & x_n
\end{pmatrix}
\begin{pmatrix}
1       & x_1       \\
\vdots  & \vdots    \\
1       & x_n
\end{pmatrix}
\right)^{-1}
\begin{pmatrix}
1   & \cdots & 1 \\
x_1 & \cdots & x_n
\end{pmatrix}
\begin{pmatrix}
\upsilon_1     \\
\vdots  \\
\upsilon_n
\end{pmatrix} \\
& =
\begin{pmatrix}
n                   & \sum_{i=1}^n x_i \\
\sum_{i=1}^n x_i    & \sum_{i=1}^n x_i^2\\
\end{pmatrix}^{-1}
\begin{pmatrix}
\sum_{i=1}^n \upsilon_i    \\
\sum_{i=1}^n x_i \upsilon_i    \\
\end{pmatrix}
\\
& =
\begin{pmatrix}
n           &  n\bar{x} \\
n\bar{x}    & \sum_{i=1}^n x_i^2\\
\end{pmatrix}^{-1}
\begin{pmatrix}
n\bar{\upsilon}    \\
\sum_{i=1}^n x_i \upsilon_i    \\
\end{pmatrix}.
\end{split}
\end{align}

Die Inverse von $X^T X$ ist gegeben durch 
\begin{equation}
\frac{1}{s_x^2}
\begin{pmatrix}
  \frac{s_x^2}{n} + \bar{x}^2
& -\bar{x}
\\
  -\bar{x}
&  1
\end{pmatrix},
\end{equation}

# Einfache lineare Regression
\footnotesize

weil \begin{align}
\begin{split}
& \frac{1}{s_x^2}
\begin{pmatrix}
  \frac{s_x^2}{n} + \bar{x}^2
& -\bar{x}
\\
  -\bar{x}
&  1
\end{pmatrix}
\begin{pmatrix}
n           &  n\bar{x} \\
n\bar{x}    & \sum_{i=1}^n x_i^2\\
\end{pmatrix}
\\
& =
\frac{1}{s_x^2}
\begin{pmatrix}
\frac{ns_x^2}{n} + n\bar{x}^2 - n \bar{x}^2
& \frac{s_x^2n\bar{x}}{n} +n\bar{x}^2\bar{x} - \bar{x}\sum_{i=1}^n x_i^2
\\
-\bar{x}n + n \bar{x}
& - n\bar{x}^2 + \sum_{i=1}^n x_i^2
\end{pmatrix} \\
& =
\frac{1}{s_x^2}
\begin{pmatrix}
s_x^2
& s_x^2\bar{x} -\bar{x} \left(\sum_{i=1}^n x_i^2 - n\bar{x}^2\right)
\\
0
& \sum_{i=1}^n x_i^2 - n\bar{x}^2
\end{pmatrix}
\\
& =
\frac{1}{s_x^2}
\begin{pmatrix}
s_x^2
& s_x^2\bar{x} - \bar{x} s_x^2
\\
0
& s_x^2
\end{pmatrix}
\\
& =
\frac{1}{s_x^2}
\begin{pmatrix}
s_x^2
& 0
\\
0
& s_x^2
\end{pmatrix}
\\
& =
\begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix}.
\end{split}
\end{align}

# Einfache lineare Regression
\footnotesize

Es ergibt sich also 
\begin{align}
\renewcommand{\arraystretch}{1.2}
\begin{split}
\hat{\beta}
= \begin{pmatrix}
  \frac{1}{n} + \frac{\bar{x}^2}{s_x^2}
& -\frac{\bar{x}}{s_x^2}
\\
  -\frac{\bar{x}}{s_x^2}
&  \frac{1}{s_x^2}
\end{pmatrix}
\begin{pmatrix}
n\bar{\upsilon}    \\
\sum_{i=1}^n x_i \upsilon_i    \\
\end{pmatrix}
& = \begin{pmatrix}
\left(\frac{1}{n} + \frac{\bar{x}^2}{s_x^2}\right)n\bar{\upsilon} - \frac{\bar{x}\sum_{i=1}^n x_i\upsilon_i}{s_x^2}   \\
\frac{\sum_{i=1}^n x_i \upsilon_i }{s_x^2} - \frac{n\bar{x}\bar{\upsilon}}{s_x^2} \\
\end{pmatrix}
\\
&
=
\begin{pmatrix}
\frac{n\bar{\upsilon}}{n} + \frac{\bar{x}^2n\bar{\upsilon}}{s_x^2}- \frac{\bar{x}\sum_{i=1}^n x_i\upsilon_i}{s_x^2}
\\
\frac{\sum_{i=1}^n x_i \upsilon_i - n\bar{x}\bar{\upsilon}}{s_x^2}    \\
\end{pmatrix}
\\
& =
\begin{pmatrix}
\bar{\upsilon} + \frac{\bar{x}n\bar{x}\bar{\upsilon} - \bar{x}\sum_{i=1}^n x_i\upsilon_i}{s_x^2}
\\
\frac{\sum_{i=1}^n x_i \upsilon_i - n\bar{x}\bar{\upsilon}}{s_x^2}    \\
\end{pmatrix}
\\
& =
\begin{pmatrix}
\bar{\upsilon} - \frac{\sum_{i=1}^n x_i\upsilon_i - n\bar{x}\bar{\upsilon}}{s_x^2}\bar{x}
\\
\frac{\sum_{i=1}^n x_i \upsilon_i - n\bar{x}\bar{\upsilon}}{s_x^2}
\end{pmatrix}
\\
& =
\begin{pmatrix}
\bar{\upsilon} - \frac{c_{x\upsilon}}{s_x^2}\bar{x}
\\
\frac{c_{x\upsilon}}{s_x^2}
\end{pmatrix}.
\end{split}
\end{align}

# Einfache lineare Regression
\vspace{1mm}
Parameterschätzung
\vspace{2mm}
\setstretch{1}
\tiny

```{r}
# Modellformulierung
library(MASS)                                    # Multivariate Normalverteilung
n          = 10                                  # Anzahl Datenpunkte
p          = 2                                   # Anzahl Betaparameter
x          = 1:n                                 # Prädiktorwerte
X          = matrix(c(rep(1,n),x), nrow = n)     # Designmatrix
I_n        = diag(n)                             # n x n Einheitsmatrix
beta       = matrix(c(0,1), nrow = p)            # wahrer, aber unbekannter, Betaparameter
sigsqr     = 1                                   # wahrer, aber unbekannter, Varianzparameter

# Datenrealisierung
y          = mvrnorm(1, X %*% beta, sigsqr*I_n)  # eine Realisierung eines n-dimensionalen ZVs

# Parameterschätzung
beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y    # Betaparameterschätzer
eps_hat    = y - X %*% beta_hat                  # Residuenvektor
sigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)     # Varianzparameterschätzer

# Ausgabe
cat("beta       : ", beta,
    "\nhat{beta}  : ", beta_hat,
    "\nsigsqr     : ", sigsqr,
    "\nhat{sigsqr}: ", sigsqr_hat)
```

# Einfache lineare Regression
\vspace{1mm}
Simulation der Schätzerunverzerrtheit
\vspace{2mm}

\setstretch{1}
\tiny
```{r}
# Modellformulierung
library(MASS)                                        # Multivariate Normalverteilung
n          = 10                                      # Anzahl Datenpunkte
p          = 2                                       # Anzahl Betaparameter
x          = 1:n                                     # Prädiktorwerte
X          = matrix(c(rep(1,n),x), nrow = n)         # Designmatrix
I_n        = diag(n)                                 # n x n Einheitsmatrix
beta       = matrix(c(0,1), nrow = p)                # wahrer, aber unbekannter, Betaparameter
sigsqr     = 1                                       # wahrer, aber unbekannter, Varianzparameter

# Frequentistische Simulation
nsim       = 1e4                                     # Anzahl Realisierungen des n-dimensionalen ZVs
beta_hat   = matrix(rep(NaN,p*nsim), nrow = p)       # \hat{\beta} Realisierungsarray
sigsqr_hat = rep(NaN,nsim)                           # \hat{sigsqr} Realisierungsarray
for(i in 1:nsim){                                    # Simulationsiterationen
  y             = mvrnorm(1, X %*% beta, sigsqr*I_n) # Datenrealisierung
  beta_hat[,i]  = solve(t(X) %*% X) %*% t(X) %*% y   # Betaparameterschätzer
  eps_hat       = y - X %*% beta_hat[,i]             # Residuenvektor
  sigsqr_hat[i] = (t(eps_hat) %*% eps_hat) /(n-p)    # Varianzparameterschätzer
}

# Ausgabe
cat("Wahrer, aber unbekannter, Betaparameter                  : ", beta,
    "\nGeschätzter Erwartungswert des Betaparameterschätzers    : ", rowMeans(beta_hat),
    "\nWahrer, aber unbekannter, Varianzparameter               : ", sigsqr,
    "\nGeschätzter Erwartungswert des Varianzparameterschätzers : ", mean(sigsqr_hat))
```


#
\large
\setstretch{2.7}
\vfill
Allgemeine Theorie

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

**Frequentistische Schätzerverteilungen**

Selbstkontrollfragen
\vfill


# Frequentistische Schätzerverteilungen
\footnotesize
\begin{theorem}[Frequentistische Verteilung des Betaparameterschätzers]
\justifying
\normalfont
Es sei
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
das ALM. Weiterhin sei
\begin{equation}
\hat{\beta} := \left(X^TX\right)^{-1}X^T\upsilon
\end{equation}
der Betaparameterschätzer. Dann gilt
\begin{equation}
\hat{\beta} \sim N\left(\beta,\sigma^2(X^T X)^{-1}\right).
\end{equation}
\end{theorem}

Bemerkungen

* \justifying Es gilt also wie bereits gesehen $\mathbb{E}(\hat{\beta}) = \beta$ und außerdem $\mathbb{C}(\hat{\beta}) = \sigma^2(X^TX)^{-1}$.
* Die Varianzen der Komponenten von $\hat{\beta}$ sind die Diagonalelemente von $\mathbb{C}(\hat{\beta})$, also
\begin{equation}
\mathbb{V}(\hat{\beta}_i) = (\sigma^2(X^TX)^{-1})_{ii} \mbox{ für } i = 1,...p.
\end{equation}
* Die Streuung von $\hat{\beta}$ hängt von $\sigma^2$ und der Designmatrix $X$ ab.
$\sigma^2$ ist ein experimentell nicht zu beinflussender wahrer, aber unbekannter, Parameter
$X$ dagegen kann so gewählt werden, um zum Beispiel die Diagonalelemente von
$\mathbb{C}(\hat{\beta})$ bei festem $\sigma^2$ zu minimieren.

# Frequentistische Schätzerverteilungen
\footnotesize
\underline{Beweis}

Das Theorem folgt direkt mit dem Theorem zur linearen Transformation von
multivariaten Normalverteilung aus Einheit (4) Normalverteilungen. Speziell
gilt hier:
\begin{equation}
\hat{\beta}
\sim
N\left(
(X^{T}X)^{-1}X^{T}X\beta,
(X^{T}X)^{-1}X^{T}(\sigma^{2}I_{n})((X^{T}X)^{-1}X^{T})^{T}
\right).
\end{equation}
Der Erwartungswertparameter vereinfacht sich dann zu
\begin{equation}
(X^{T}X)^{-1}X^{T}X\beta = \beta.
\end{equation}
Der Kovarianzmatrixparamter vereinfacht sich wie folgt
\begin{align}
\begin{split}
(X^{T}X)^{-1}X^{T}(\sigma^{2}I_{n})((X^{T}X)^{-1}X^{T})^{T}
&  =(X^{T}X)^{-1}X^{T}(\sigma^{2}I_{n})X(X^{T}X)^{-1} 				\\
&  =\sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}						\\
&  =\sigma^{2}(X^{T}X)^{-1}.
\end{split}
\end{align}
Dabei folgt hier die erste Gleichung aus der Tatsache, dass sowohl $X^{T}X$ als auch
ihre Inverse $(X^{T}X)^{-1}$ symmetrische Matrizen sind. 

Damit gilt dann insgesamt aber sofort
\begin{equation}
\hat{\beta} \sim N\left(\beta,\sigma^2(X^T X)^{-1}\right).
\end{equation}

# Frequentistische Schätzerverteilungen
Beispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen
\vspace{1mm}

\small
Es sei
\begin{equation}
\upsilon \sim N(X\beta,\sigma^2 I_n)
\mbox{ mit }
X := 1_n \in \mathbb{R}^{n\times 1},
\beta := \mu \in \mathbb{R}
\mbox{ und } \sigma^2 > 0.
\end{equation}
das ALM Szenario unabhängiger und identisch normalverteilter Zufallsvariablen.
Wir haben bereits gesehen, dass $\hat{\beta} = \bar{\upsilon}$. Das Theorem zur Frequentistischen
Verteilung des Betaparameterschätzers impliziert damit
\begin{equation}
\bar{\upsilon} \sim N\left(\mu, \frac{\sigma^2}{n}\right).
\end{equation}
Das Stichprobenmittel von $n$ unabhängigen und identisch normalverteilten Zufallsvariablen
mit Erwartungswertparameter $\mu$ und Varianzparameter $\sigma^2$ ist also normalverteilt
mit Erwartungswertparameter $\mu$ und Varianzparameter  $\sigma^2/n$. Wir haben diese
Tatsache bereits in Einheit (8) Transformationen der Normalverteilungen in Wahrscheinlichkeitstheorie
und Frequentistische Inferenz unter dem Begriff der Mittelwertstransformation kennengelernt.

# Frequentistische Schätzerverteilungen
Beispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen
\vspace{2mm}

\footnotesize
```{r}
# Modellformulierung
library(MASS)                                      # Multivariate Normalverteilung  
n        = 12                                      # Anzahl von Datenpunkten
p        = 1                                       # Anzahl von Betparametern
X        = matrix(rep(1,n), nrow = n)              # Designmatrix
I_n      = diag(n)                                 # n x n Einheitsmatrix
beta     = 2                                       # wahrer, aber unbekannter, Betaparameter
sigsqr   = 1                                       # wahrer, aber unbekannter, Varianzparameter

# Frequentistische Simulation
nsim     = 1e4                                     # Anzahl Realisierungen des n-dimensionalen ZVs
beta_hat = rep(NaN,nsim)                           # \hat{\beta} Realisierungsarray
for(i in 1:nsim){
  y           = mvrnorm(1, X %*% beta, sigsqr*I_n) # eine Realisierung eines n-dimensionalen ZVs
  beta_hat[i] = solve(t(X) %*% X) %*% t(X) %*% y   # \hat{\beta} = (X^T)X^{-1}X^T\upsilon
}
```

```{r, echo = F, eval = F}
# figure setup
library(latex2exp)
dev.new()
fig = par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(3,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)

# density
b_hat_min  = 0.5
b_hat_max  = 3.5
b_hat_res  = 1e3
b_hat      = seq(b_hat_min, b_hat_max, len = b_hat_res)
p_beta_hat = dnorm(b_hat, beta, sqrt(sigsqr/n))

# histogram
hist(
beta_hat,
breaks = 50,
col   = "gray90",
prob  = TRUE,
xlab  = TeX("$\\bar{\\upsilon}$"),
ylab  = "",
xlim  = c(0.5,3.5),
ylim  = c(0,1.5),
main  = "")

# density
lines(
b_hat,
p_beta_hat,
lwd   = 2,
col   = "darkorange")

# print
dev.copy2pdf(
file   = file.path(getwd(), "6_Abbildungen", "alm_6_beta_hat_1.pdf"),
width  = 5,
height = 5)

```

# Schätzerverteilungen
Beispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen
\vspace{3mm}
\center

$\bar{\upsilon} \sim N\left(\mu,\frac{\sigma^2}{n}\right)$
\vspace{-2mm}
```{r, echo = FALSE, out.width = "50%"}
knitr::include_graphics("6_Abbildungen/alm_6_beta_hat_1.pdf")
```
\vfill

# Schätzerverteilungen
Beispiel (2) Einfache lineare Regression
\vspace{1mm}

\small
Es sei
\begin{equation}
\upsilon \sim N(X\beta, \sigma^2I_n) \mbox{ mit }
\begin{pmatrix}
1 		& x_1		\\
\vdots 	& \vdots	\\
1 		& x_n
\end{pmatrix}
\in \mathbb{R}^{n\times 2},
\beta
\in \mathbb{R}^2,
\sigma^2 > 0.
\end{equation}
das ALM Szenario der einfachen linearen Regression. Wir haben bereits gesehen,
dass
\begin{equation}
\sigma^2(X^TX)^{-1}
=
\frac{\sigma^2}{s_x^2}
\begin{pmatrix}
  \frac{s_x^2}{n} + \bar{x}^2
& -\bar{x}
\\
  -\bar{x}
&  1
\end{pmatrix}
\mbox{ mit  }
s_x^2 := \sum_{i=1}^n (x_i - \bar{x})^2.
\end{equation}
Die Varianz des Offsetparameterschätzers hängt also sowohl von der Summe der
quadrierten Differenzen und dem Stichprobenmittel der unabhängigen Variablen $x_1,...,x_n$ ab,
wohingegen die Varianz des Steigungsparameterschätzers nur von der Summe der quadrierten
Differenzen der $x_1,...,x_n$ abhängt. Die Kovarianz von Offset- und Steigungsparameterschätzern
hängt vom Mittelwert der $x_1,...,x_n$ ab.

# Schätzerverteilungen
Beispiel (2) Einfache lineare Regression
\vspace{1mm}
\setstretch{1.2}
\footnotesize
```{r}
# Modellformulierung
library(MASS)                                         # Multivariate Normalverteilung
n        = 10                                         # Anzahl von Datenpunkten
p        = 2                                          # Anzahl von Betparametern
x        = 1:n                                        # Prädiktorwerte
X        = matrix(c(rep(1,n),x), nrow = n)            # Designmatrix
I_n      = diag(n)                                    # n x n Einheitsmatrix
beta     = matrix(c(0,1), nrow = p)                   # wahrer,aber unbekannter,Betaparameter
sigsqr   = .5                                         # wahrer,aber unbekannter,Varianzparameter

# Frequentistische Simulation
nsim     = 10                                         # Anzahl Realisierungen n-dimensionaler ZV
y        = matrix(rep(NaN,n*nsim), nrow = n)          # y Realisierungsarray
beta_hat = matrix(rep(NaN,p*nsim), nrow = p)          # \hat{\beta} Realisierungsarray
for(i in 1:nsim){
  y[,i]        = mvrnorm(1, X %*% beta, sigsqr*I_n)   # eine Realisierung n-dimensionaler ZV
  beta_hat[,i] = solve(t(X) %*% X) %*% t(X) %*% y[,i] # \hat{\beta} = (X^T)X^{-1}X^T\upsilon
}
```

# Schätzerverteilungen
Beispiel (2) Einfache lineare Regression

```{r, eval = F, echo = F}
library(mvtnorm)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Graulevel
gfun        = colorRampPalette(c("grey20", "grey80"))
greys       = gfun(nsim)


# Datenrealisierungen
xlimits     = c(0,11)
ylimits     = c(-3,13)
plot(
x,
X %*% beta,
type        = "b",
lty         = 2,
pch         = 1,
col         = "white",
xlab        = "x",
ylab        = "y",
xlim        = xlimits,
ylim        = ylimits)
for(i in 1:nsim){
  lines(
  type = "b",
  x,
  y[,i],
  col = greys[i],
  pch = 16)
}

# Wahrscheinlichkeitsdichtefunktion
b_hat_min  = -2
b_hat_max  = 2
b_hat_res  = 1e3
b_hat_1    = seq(b_hat_min, b_hat_max, len = b_hat_res)
b_hat_2    = seq(b_hat_min, b_hat_max, len = b_hat_res)
b_hat      = expand.grid(b_hat_1,b_hat_2)
mu         = beta
Sigma      = sigsqr*solve(t(X) %*% X)
p_beta_hat = matrix(dmvnorm(as.matrix(b_hat), mu, Sigma), nrow = b_hat_res)

# Visualisierung
contour(
b_hat_1,
b_hat_2,
p_beta_hat,
xlim      = c(-1.5,1.5),
ylim      = c(0.5,1.5),
xlab      = TeX("$\\hat{\\beta}_1$"),
ylab      = TeX("$\\hat{\\beta}_2$"),
nlevels   = 5,
col       = "orange")
for(i in 1:nsim){
  points(
  type = "p",
  beta_hat[1,i],
  beta_hat[2,i],
  col = greys[i],
  pch = 16)
}

# Speichern
dev.copy2pdf(
file        = file.path("./6_Abbildungen/alm_6_beta_hat_2.pdf"),
width       = 7,
height      = 3.5)
```

\vspace{4mm}
\center
$\quad\quad\quad\quad \upsilon \sim (X\beta,\sigma^2I_n)$ \hspace{3cm} $\hat{\beta} \sim N(\beta,\sigma^2(X^TX)^{-1})$

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("6_Abbildungen/alm_6_beta_hat_2.pdf")
```

# Schätzerverteilungen
\footnotesize
\begin{theorem}[Frequentistische Verteilung des Varianzparameterschätzers]
\normalfont
\justifying
Es sei
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
das ALM. Weiterhin sei
\begin{equation}
\hat{\sigma}^2 = \frac{(\upsilon -  X\hat{\beta})^T(\upsilon -  X\hat{\beta})}{n-p}
\end{equation}
der Varianzparameterschätzer. Dann gilt
\begin{equation}
\frac{n-p}{\sigma^2}\hat{\sigma}^2 \sim \chi^2(n-p)
\end{equation}
\end{theorem}

Bemerkungen

* \justifying Wir verzichten auf einen Beweis. Da es sich bei $(\upsilon-X\hat{\beta})^T(\upsilon-X\hat{\beta})$ 
um eine Summe quadrierter normalverteilter Zufallsvariablen handelt, liegt die 
$\chi^2$-Verteilung im Lichte der $\chi^2$ Transformation aus Einheit (8) Transformationen der Normalverteilung
in Wahrscheinlichkeitstheorie und Frequentistische Inferenz zumindest nahe.

# Schätzerverteilungen
Beispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen
\vspace{1mm}

\small
Es sei
\begin{equation}
\upsilon \sim N(X\beta,\sigma^2 I_n)
\mbox{ mit }
X := 1_n \in \mathbb{R}^{n\times 1},
\beta := \mu \in \mathbb{R}
\mbox{ und } \sigma^2 > 0.
\end{equation}
das ALM Szenario unabhängiger und identisch normalverteilter Zufallsvariablen.
Wir haben bereits gesehen, dass in diesem Fall $\hat{\beta}$ mit dem
Stichprobenmittel $\bar{\upsilon}$ identisch ist und dass $\hat{\sigma}^2$ mit der
Stichprobenvarianz $s^2_\upsilon$ übereinstimmt.

In Einheit (11) Konfidenzintervalle von  Wahrscheinlichkeitstheorie und
Frequentistische Inferenz hatten wir für den Fall von $n$ unabhängig und identisch
normalverteilten Zufallsvariablen die Statistik
\begin{equation}
U := \frac{n-1}{\sigma^2}S^2
\end{equation}
definiert und festgehalten, dass
\begin{equation}
U \sim \chi^2(n-1).
\end{equation}
Offenbar ist $U$ für $p = 1$ mit der im obigen Theorem betrachten Zufallsvariable
$\frac{n-p}{\sigma^2}\hat{\sigma}^2$ identisch.

# Schätzerverteilungen
Beispiel (2) Einfache lineare Regression
\vspace{2mm}
\tiny
```{r}
# Modellfumuliertung
library(MASS)                                          # multivariate Normalverteilung
n          = 10                                        # Anzahl von Datenpunkten
p          = 2                                         # Anzahl von Betparametern
x          = 1:n                                       # Prädiktorwerte
X          = matrix(c(rep(1,n),x), nrow = n)           # Designmatrix
I_n        = diag(n)                                   # n x n Einheitsmatrix
beta       = matrix(c(0,1), nrow = p)                  # wahrer,aber unbekannter,Betaparameter
sigsqr     = .5                                        # wahrer,aber unbekannter,Varianzparameter

# Frequentistische Simulation
nsim       = 1e3                                       # Anzahl Realisierungen n-dimensionaler ZV
y          = matrix(rep(NaN,n*nsim), nrow = n)         # y Realisierungsarray
beta_hat   = matrix(rep(NaN,p*nsim), nrow = p)         # \hat{\beta} Realisierungsarray
sigsqr_hat = rep(NaN, nsim)                            # \hat{\sigma}^2 Realisierungsarray
for(i in 1:nsim){
  y[,i]         = mvrnorm(1, X %*% beta, sigsqr*I_n)   # eine Realisierung n-dimensionaler ZV
  beta_hat[,i]  = solve(t(X) %*% X) %*% t(X) %*% y[,i] # \hat{\beta}    = (X^T)X^{-1}X^T\upsilon
  eps_hat       = y[,i] - X %*% beta_hat[,i]           # \hat{\eps}     = \upsilon-X\hat{\beta}
  sigsqr_hat[i] = (t(eps_hat) %*% eps_hat)/(n-p)       # \hat{\sigma}^2 =\hat{\eps}^T\hat{\eps}/(n-p)
}
U = ((n-p)/sigsqr)*sigsqr_hat                          # \chi^2 verteilte Zufallsvariable
```

# Schätzerverteilungen
Beispiel (2) Einfache lineare Regression

```{r, eval = F, echo = F}
library(mvtnorm)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Datenrealisierungen
xlimits     = c(0,11)
ylimits     = c(-5,15)
plot(
x,
X %*% beta,
type        = "b",
lty         = 2,
pch         = 1,
col         = "white",
xlab        = "x",
ylab        = "y",
xlim        = xlimits,
ylim        = ylimits)
for(i in 1:nsim){
  lines(
  type = "l",
  x,
  y[,i],
  col = "grey")
}

# density
xlimits     = c(0,30)
ylimits     = c(0,.12)
u_min  = xlimits[1]
u_max  = xlimits[2]
u_res  = 1e3
u      = seq(u_min, u_max, len = u_res)
p_u    = dchisq(u,n-p)

# histogram
hist(
U,
col   = "gray90",
prob  = TRUE,
xlab  = TeX("$((n-p)/\\sigma^2)\\hat{\\sigma}^2$"),
ylab  = "",
xlim  = xlimits,
ylim  = ylimits,
main   = "")

# density
lines(
u,
p_u,
lwd   = 2,
col   = "darkorange")

# Speichern
dev.copy2pdf(
file        = file.path("./6_Abbildungen/alm_6_sigsqr_hat_1.pdf"),
width       = 7,
height      = 3.5)
```
\vspace{4mm}
\center
$\quad\quad  \upsilon \sim (X\beta,\sigma^2I_n)$ \hspace{3cm} $\frac{n-p}{\sigma^2}\hat{\sigma}^2 \sim \chi^2(n-p)$

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("6_Abbildungen/alm_6_sigsqr_hat_1.pdf")
```

#
\large
\setstretch{2.7}
\vfill
Allgemeine Theorie

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

Frequentistische Schätzerverteilungen

**Selbstkontrollfragen**
\vfill

# Selbstkontrollfragen
\footnotesize
\setstretch{3}

1.  Geben Sie das Theorem zum Betaparameterschätzer wieder.
1.  Warum ist der Betaparameterschätzer ein Maximum-Likelihood Schätzer?
1.  Geben Sie das Theorem zum Varianzparameterschätzer wieder-
1.  Geben Sie die Parameterschätzer bei $n$ unabhängigen und identisch normalverteilten Zufallsvariablen an.
1.  Geben Sie die Parameterschätzer bei einfacher linearer Regression an.
1.  Geben Sie das Theorem zur Verteilung des Betaparameterschätzers wieder.
1.  Geben Sie das Theorem zur Verteilung des Varianzparameterschätzers wieder.

# Referenzen
\footnotesize
