---
fontsize: 8pt
bibliography: 7_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 7_header.tex
---


```{r, include = F}
source("7_R_common.R")
fdir        = file.path(getwd(), "7_Abbildungen")                                # Abbildungsverzeichnis
```


#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("7_Abbildungen/alm_7_otto.png")
```

\vspace{2mm}

\huge
Allgemeines Lineares Modell
\vspace{6mm}

\large
BSc Psychologie SoSe 2023

\vspace{6mm}
\normalsize
Prof. Dr. Dirk Ostwald

#  {.plain}
\center
\huge
\vfill
\noindent (7) T-Statistiken
\vfill

#
\large

Naturwissenschaft \vspace{7mm}

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("7_Abbildungen/alm_7_wissenschaft.pdf")
```

#
\vspace{1mm}
\normalsize
Modellformulierung
\vspace{1mm}
\small
\begin{equation}
\upsilon = X\beta + \varepsilon, \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
\vspace{5mm}

\normalsize
Modellschätzung
\small
\begin{equation}
\hat{\beta} = (X^TX)^{-1} X^T\upsilon,  \hat{\sigma}^2 = \frac{(\upsilon-X\hat{\beta})^T(\upsilon-X\hat{\beta})}{n-p}
\end{equation}
\vspace{4mm}

\normalsize
Modellevaluation
\small
\begin{equation}
T = \frac{c^T\hat{\beta} - c^T\beta_0}{\sqrt{\hat{\sigma}^2c^T(X^TX)^{-1}c}},
F = \frac{(\hat{\varepsilon}_1^T\hat{\varepsilon}_1 - \hat{\varepsilon}^T\hat{\varepsilon})/p_2}{\hat{\varepsilon}^T\hat{\varepsilon}/(n-p)}
\end{equation}

#
Standardprobleme Frequentistischer Inferenz

\small
\vspace{2mm}
\noindent (1) Parameterschätzung

Ziel der Parameterschätzung ist es, einen möglichst guten Tipp für wahre, aber unbekannte,
Parameterwerte oder Funktionen dieser abzugeben, typischerweise mithilfe von Daten.

\vspace{2mm}
\noindent (2) Konfidenzintervalle

Ziel der Bestimmung von Konfidenzintervallen ist es, basierend auf der angenommenen
Verteilung der Daten eine quantitative Aussage über die mit Schätzwerten assoziierte
Unsicherheit zu treffen.

\vspace{2mm}
\noindent (3) Hypothesentests

Ziel des Hypothesentestens ist es, basierend auf der angenommenen
Verteilung der Daten in einer möglichst zuverlässigen Form zu entscheiden, ob ein
wahrer, aber unbekannter Parameterwert in einer von zwei sich gegenseitig
ausschließenden Untermengen des Parameterraumes liegt.

#
\center
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("7_Abbildungen/alm_7_frequentistische_inferenz.pdf")
```
\center
\footnotesize
$\theta := (\beta,\sigma^2)$,
$\Theta := \mathbb{R}^p \times \mathbb{R}_{>0}$
$\mathbb{P}_\theta(y) := \mathbb{P}_{\beta,\sigma^2}(y)$ mit WDF $p_{\beta,\sigma^2}(\upsilon) := N(\upsilon;X\beta,\sigma^2I_n)$

#
\small
Standardannahmen Frequentistischer Inferenz

\footnotesize
\setstretch{1.2}
Gegeben sei das Allgemeine Lineare Modell. Es wird angenommen, dass ein
vorliegender Datensatz eine der möglichen Realisierungen der Daten des Modells ist.
Aus Frequentistischer Sicht kann man unendlich oft Datensätze basierend auf einem
Modell generieren und zu jedem Datensatz Schätzer oder Statistiken auswerten, z.B.
den Betaparameterschätzer
\vspace{1mm}

\begin{itemize}
\item[] Datensatz (1) : $y^{(1)} = \left(y_1^{(1)}, y_2^{(1)}, ...,y_n^{(1)}\right)^T$ 	mit $\hat{\beta}^{(1)} = (X^TX)^{-1}X^Ty^{(1)}$
\item[] Datensatz (2) : $y^{(2)} = \left(y_1^{(2)}, y_2^{(2)}, ...,y_n^{(2)}\right)^T$ 	mit $\hat{\beta}^{(2)} = (X^TX)^{-1}X^Ty^{(2)}$
\item[] Datensatz (3) : $y^{(3)} = \left(y_1^{(3)}, y_2^{(3)}, ...,y_n^{(3)}\right)^T$ 	mit $\hat{\beta}^{(3)} = (X^TX)^{-1}X^Ty^{(3)}$
\item[] Datensatz (4) : $y^{(4)} = \left(y_1^{(4)}, y_2^{(4)}, ...,y_n^{(4)}\right)^T$ 	mit $\hat{\beta}^{(4)} = (X^TX)^{-1}X^Ty^{(4)}$
\item[] Datensatz (5) : $y^{(5)} = ...$
\end{itemize}

\vspace{1mm}
Um die Qualität statistischer Methoden zu beurteilen betrachtet die Frequentistische
Statistik die Wahrscheinlichkeitsverteilungen von Schätzern und Statistiken
unter Annahme der Datenverteilung. Was zum Beispiel ist die Verteilung von
$\hat{\beta}^{(1)}$, $\hat{\beta}^{(2)}$, $\hat{\beta}^{(3)}$, $\hat{\beta}^{(4)}$, ... also die
Verteilung der Zufallsvariable $\hat{\beta} := (X^TX)^{-1}X^T\upsilon$? Wenn eine statistische
Methode im Sinne der Frequentistischen Standardannahmen "gut" ist, dann heißt das
also, dass sie bei häufiger Anwendung "im Mittel gut" ist. Im Einzelfall, also
im Normalfall nur eines vorliegenden Datensatzes, kann sie auch "schlecht" sein.

#
Überblick

\small
* \justifying In diesem Abschnitt führen wir T-Statistiken als Maße zur Evaluation von 
Betaparameterschätzern im ALM ein. T-Statistiken quantifizieren dabei die 
geschätzten Effekte des Betaparameterschätzers in bezug zur durch den 
Varianzparameterschätzer geschätzten Residualvariabilität. Der Wert einer 
T-Statistik ist also zunächst einmal einfach als Signal-zu-Rauschen Verhältnis 
(Signal-to-Noise Ratio) zu verstehen. 

* \justifying T-Statistiken erlauben weiterhin die Evaluation von Linearkombinationen der 
Komponenten des Betaparameterschätzers im Sinne Frequentistischer Konfidenzinteralle 
und Hypothesentests. Wir betrachten hier zunächst nur die funktionale Form von 
T-Statistiken und ihre Frequentistische Verteilung zum Zwecke der 
Konfidenzintervallbestimmung. Der Einsatz von T-Teststatistiken im Rahmen von 
Einstich- und Zweistichproben T-Tests ist das Thema von Einheit (9) T-Tests.


#   
\vfill
\large
\setstretch{3}
T-Zufallsvariablen

T-Statistiken

Konfidenzintervalle

Selbstkontrollfragen
\vfill


#  
\vfill
\large
\setstretch{3}
**T-Zufallsvariablen**

T-Statistiken

Konfidenzintervalle

Selbstkontrollfragen
\vfill

# T-Zufallsvariablen
\footnotesize
\begin{definition}[$t$-Zufallsvariable]
\justifying
$\xi$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}$ und WDF
\begin{equation}
p_\xi : \mathbb{R} \to \mathbb{R}_{>0}, x \mapsto p_\xi(x)
:= \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}
\left(1 + \frac{x^2}{n} \right)^{-\frac{n+1}{2}},
\end{equation}
wobei $\Gamma$ die Gammafunktion bezeichne. Dann sagen wir, dass $\xi$ einer
$t$-Verteilung mit Freiheitsgradparameter $n$ unterliegt und nennen $\xi$ eine $t$-Zufallsvariable
mit Freiheitsgradparameter $n$. Wir kürzen dies mit $\xi \sim t(n)$ ab. Die WDF einer
$t$-Zufallsvariable bezeichnen wir mit $t(\cdot ;n)$. Die KVF und inverse KVF einer 
nichtzentralen $t$-Zufallsvariable bezeichnen wir mit $\Psi(\cdot;n)$ 
und $\Psi^{-1}(\cdot;n)$, respektive.
\end{definition}

Bemerkungen

* Die Verteilung ist um 0 symmetrisch
* Steigendes $n$ verschiebt Wahrscheinlichkeitsmasse aus den Ausläufen zum Zentrum.
* Ab $n = 30$ gilt $t(n) \approx N(0,1)$.


# T-Zufallsvariablen

```{r, echo = F, eval = F}
# Modellformulierung
t_min   = -5                                                                     # minimum t-value
t_max   = 5                                                                      # maximum t-value
t_res   = 1e3                                                                    # t-space resolution
t       = seq(t_min,t_max, len = t_res)                                          # t-space
n       = c(2,3,5,10,30)                                                         # degrees of freedom

# Visualisierung
library(latex2exp)
dev.new()
par(
family     = "sans",
pty        = "m",
bty        = "l",
lwd        = 1,
las        = 1,
mgp        = c(2,1,0),
xaxs       = "i",
yaxs       = "i",
font.main  = 1,
cex        = 1.1,
cex.main   = 1.1)


matplot(
t,
matrix(c(dt(t,n[1]),
         dt(t,n[2]),
         dt(t,n[3]),
         dt(t,n[4]),
         dt(t,n[5])),
         ncol = 5),
type         = "l",
lty          = 1,
lwd          = 2,
col          = c("gray10", "gray20", "gray50", "gray70", "gray90"),
ylim         = c(0,.4),
xlim         = c(t_min,t_max),
ylab         = " ",
xlab         = " ",
main         = TeX("$\\t(\\cdot;n)$"))

legend(
2,
.4,
c("n = 2", "n = 3", "n = 5", "n = 10", "n = 30"),
lty         = 1,
lwd         = 2,
col         =   c("gray10", "gray20", "gray50", "gray70", "gray90"),
bty         = "n",
cex         = 1.1,
y.intersp   = 1.6)

dev.copy2pdf(
file   = file.path("./7_Abbildungen/alm_7_t_wdf.pdf"),
width  = 6,
height = 5)
```

\vfill
Wahrscheinlichkeitsdichtefunktionen von $t$-Zufallsvariablen
\vspace{5mm}
```{r, echo = FALSE, out.width="70%"}
knitr::include_graphics("7_Abbildungen/alm_7_t_wdf.pdf")
```
\vspace{-3mm}
\footnotesize


# T-Zufallsvariablen
\footnotesize
\begin{theorem}[$T$-Transformation]
\justifying
\normalfont
$Z \sim N(0,1)$ sei eine  $Z$-Zufallsvariable, $U \sim \chi^2(n)$ sei eine
$\chi^2$-Zufallsvariable Freiheitsgradparameter $n$, und $Z$ und $U$ seien
unabhängige Zufallsvariablen. Dann ist die Zufallsvariable
\begin{equation}
T := \frac{Z}{\sqrt{U/n}}
\end{equation}
eine $t$-verteilte Zufallsvariable mit Freiheitsgradparameter $n$, es gilt also $T \sim t(n)$.
\end{theorem}

Bemerkungen

* Das Theorem geht auf @student_1908 zurück.
* Das Theorem ist eines der zentralen Resultate der Frequentistischen Statistik.
* @zabell_2008 gibt hierzu einen historischen Überblick.

# T-Zufallsvariablen

\footnotesize
\begin{definition}[Nichtzentrale $t$-Zufallsvariable]
\justifying
$\xi$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}$ und WDF
\begin{multline}
p_\xi : \mathbb{R} \to \mathbb{R}_{>0}, x \mapsto p_\xi(x) :=
\frac{1}{2^{\frac{n-1}{2}}\Gamma\left(\frac{n}{2} \right)(n \pi)^{\frac{1}{2}}} \\
\times \int_{0}^\infty \tau^{\frac{n-1}{2}} \exp\left(-\frac{\tau}{2}\right)
\exp\left(-\frac{1}{2}\left(x \left(\frac{\tau}{n}\right)^{\frac{1}{2}} - \delta \right)^2 \right)\,d\tau.
\end{multline}
Dann sagen wir, dass $\xi$ einer nichtzentralen $t$-Verteilung mit
Nichtzentralitätsparameter $\delta$ und Freiheitsgradparameter $n$ unterliegt
und nennen $\xi$ eine \textit{nichtzentrale $t$-Zufallsvariable mit Nichtzentralitätsparameter $\delta$ und
Freiheitsgradparameter $n$}. Wir kürzen dies mit $\xi \sim t(\delta, n)$ ab. Die WDF einer
nichtzentralen $t$-Zufallsvariable  bezeichnen wir mit
$t(\cdot;\delta,n)$. Die KVF und inverse KVF einer nichtzentralen $t$-Zufallsvariable
bezeichnen wir mit $\Psi(\cdot; \delta, n)$ und $\Psi^{-1}(\cdot; \delta, n)$, respektive.
\end{definition}

Bemerkungen

* Eine nichtzentrale $t$-Zufallsvariable mit $\delta = 0$ ist eine $t$-Zufallsvariable.
* Es gilt also $t(\cdot;0,n) = t(\cdot;n)$.
* Die funktionale Form der WDF findet sich zum Beispiel in @lehmann_1986, Seite 254, Gl. (80).


# T-Zufallsvariablen

Wahrscheinlichkeitsdichtefunktionen nichtzentraler $t$-Zufallsvariablen
\vspace{4mm}

```{r, echo = F, eval = F}

# Modellformulierung
options(warn=-1)                                                                 # warning off
t_min     = -5                                                                   # Minimum T-Wert
t_max     = 30                                                                   # Maximum T-Wert
t_res     = 1e3                                                                  # T-Wert Auflösung
t         = seq(t_min, t_max, len = t_res)                                       # T-Raum
delta     = c(0,5,15)                                                            # Nichtzentralitätsparameter
n         = c(5, 30)                                                             # Freiheitsgrade
p         = cbind(
            matrix(dt(t, n[1], delta[1]),nrow=length(t)),
            matrix(dt(t, n[2], delta[1]),nrow=length(t)),
            matrix(dt(t, n[1], delta[2]),nrow=length(t)),
            matrix(dt(t, n[2], delta[2]),nrow=length(t)),
            matrix(dt(t, n[1], delta[3]),nrow=length(t)),
            matrix(dt(t, n[2], delta[3]),nrow=length(t)))

# Visualisierung
dev.new()
library(latex2exp)
graphics.off()
par(
family      = "sans",
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
matplot(
t,
p,
type        = "l",
lty         = c(1,2,1,2,1,2),
col         = c("gray10", "gray10", "gray50", "gray50", "gray70", "gray70"),
lwd         = 2,
xlab        = "",
ylab        = "",
ylim        = c(0,.4),
main        = TeX("$t(\\cdot;\\delta,n)$"))
legend(
18,
.4,
c(TeX("$\\delta = 0 , n = 5$"),
  TeX("$\\delta = 0 , n = 30$"),
  TeX("$\\delta = 5 , n = 5$"),
  TeX("$\\delta = 5 , n = 30$"),
  TeX("$\\delta = 15, n = 5$"),
  TeX("$\\delta = 15, n = 30$")),
lty         = c(1,2,1,2,1,2),
col         = c("gray10", "gray10", "gray50", "gray50", "gray70", "gray70"),
lwd         = 2,
bty         = "n",
seg.len     = 1.75)
dev.copy2pdf(
file        = file.path("./7_Abbildungen/alm_7_nichtzentrale_t_verteilung.pdf"),
width       = 7,
height      = 4.5)
```

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("7_Abbildungen/alm_7_nichtzentrale_t_verteilung.pdf")
```

# T-Zufallsvariablen
\footnotesize
\begin{theorem}[Nichtzentrale T-Transformation]
\normalfont
\justifying
$\xi \sim N(\mu,1)$ sei eine normalverteilte Zufallsvariable, $U \sim \chi^2(n)$
sei eine $\chi^2$ Zufallsvariable mit Freiheitsgradparameter $n$, und $\xi$ und
$U$ seien unabhängige Zufallsvariablen. Dann ist die Zufallsvariable
\begin{equation}
T := \frac{\xi}{\sqrt{U/n}}
\end{equation}
eine nichtzentrale $t$-Zufallsvariable mit Nichtzentralitätsparameter $\mu$ und
Freiheitsgradparameter $n$, es gilt also $T \sim t(\mu,n)$
\end{theorem}

Bemerkung

* Wir verzichten auf einen Beweis.

# 
\vfill
\large
\setstretch{3}
T-Zufallsvariablen

**T-Statistiken**

Konfidenzintervalle

Selbstkontrollfragen
\vfill

# T-Statistiken
\footnotesize
\begin{definition}[T-Statistik]
Es sei 
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N\left(0_n,\sigma^2I_n\right) 
\end{equation}
das ALM. Weiterhin seien 
\begin{equation}
\hat{\beta} := (X^TX)^{-1}X^T\upsilon \mbox{ und } \hat{\sigma}^2 := \frac{(\upsilon - X\hat{\beta})^T(\upsilon - X\hat{\beta})}{n-p} 
\end{equation}
die Betaparameter- und Varianzparameterschätzer, respektive. Dann ist für einen 
\textit{Kontrastgewichtsvektor} $c \in \mathbb{R}^p$ und einen Parameter 
$\beta_0 \in \mathbb{R}^p$ die \textit{T-Statistik} definiert als 
\begin{equation}
T := \frac{c^T\hat{\beta} - c^T\beta_0}{\sqrt{\hat{\sigma}^2c^T(X^TX)^{-1}c}}. 
\end{equation}
\end{definition}

Bemerkungen

* Die T-Statistik hängt via $\hat{\beta}$ und $\hat{\sigma}^2$ von den Daten $\upsilon$ ab.
* Der Kontrastgewichtsvektor projiziert $\hat{\beta}$ auf einen Skalar $c^T\hat{\beta} \in \mathbb{R}$.
* Die Wahl $p$-dimensionaler Einheitsvektoren für $c$ erlaubt die Auswahl einzelner Komponenten von $\hat{\beta}$ bzw. $\beta_0$.
* Eine generelle Wahl von $c$ erlaubt die Evaluation beliebiger Linearkombinationen von $\hat{\beta}$ bzw. $\beta_0$.

# T-Statistiken

\footnotesize
Bemerkungen (fortgeführt)

Die Wahl von $\beta_0 \in \mathbb{R}^p$ erlaubt es, die T-Statistik unterschiedlich einzusetzen:

* \justifying Wählt man $\beta_0 := 0_p$, so erhält man mit der T-Statistik eine Deskriptivstatistik, 
die es erlaubt, geschätzte Regressoreffekte, also Komponenten oder Linearkombinationen von $\hat{\beta}$,
im Sinne eines Signal-zu-Rauschen Verhältnisses in Bezug zu der durch $\hat{\sigma}^2$ 
quantifizierten Residualdatenvariabilität zu setzen. Der Nenner der T-Statistik
stellt dabei sicher, dass insbesondere die adequate (Ko)Standardabweichung der
entsprechenden Betaparameterkomponentenkombination als Bezugsgröße dient, da es
sich bei $\hat{\sigma}^2\left(X^TX\right)^{-1}$ bekanntlich um die Kovarianz des
Betaparameterschätzers handelt. Folgende erste Intuition ist in diesem Kontext hilfreich:
\begin{equation}
T = \frac{\mbox{Geschätzte Effektstärke}}{\mbox{Geschätzte stichprobenumfangskalierte Datenvariabilität}}
\end{equation}

* \justifying Wählt man für $\beta_0 = \beta$, also den wahren, aber unbekannten, Betaparameterwert,
so eröffnet die T-Statistik die Möglichkeit, für einzelen Komponenten des
Betaparametervektors Konfidenzintervalle zu bestimmen. 

* \justifying Deklariert man schließlich $\beta_0 \in \Theta_0$ im Kontext eines Testszenarios 
als das Element einer Nullhypothese $\Theta_0$, so eröffnet die T-Statistik die 
Hypothesentest-basierte Inferenz über Betaparameterkomponenten und ihrer Linearkombinationen. 
des ALMs. 

# T-Statistiken
\vspace{1mm}
\footnotesize
\begin{theorem}[T-Statistik]
\normalfont
\justifying
Es sei 
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n) 
\end{equation}
das ALM. Weiterhin seien 
\begin{equation}
\hat{\beta} := (X^TX)^{-1}X^T\upsilon \mbox{ und } \hat{\sigma}^2 := \frac{(\upsilon-X\hat{\beta})^T(\upsilon-X\hat{\beta})}{n-p} 
\end{equation}
die Betaparameter- und Varianzparameterschätzer, respektive. Schließlich sei für 
einen Kontrastgewichtsvektor $c \in \mathbb{R}^p$ und einen Parameter 
$\beta_0 \in \mathbb{R}^p$ 
\begin{equation}
T := \frac{c^T\hat{\beta} - c^T\beta_0}{\sqrt{\hat{\sigma}^2 c^T(X^TX)^{-1}c}}  
\end{equation}
die T-Statistik. Dann gilt 
\begin{equation}
T \sim t(\delta, n-p) \mbox{ mit } \delta := \frac{c^T\beta - c^T\beta_0}{\sqrt{\sigma^2 c^T(X^TX)^{-1}c}}. 
\end{equation}
\end{theorem}
\vspace{-2mm}

Bemerkungen
\vspace{-2mm}

* Wir verzichten auf einen Beweis.
* $T$ ist eine Funktion der Parameterschätzer, $\delta$ ist eine Funktion der wahren, aber unbekannten, Parameter
* Für $c^T\beta = c^T\beta_0$, also bei Zutreffen der Nullhypothese, gilt $\delta = 0$ und damit $T \sim t(n-p)$.
* Für $c^T\beta \neq c^T\beta_0$ kann die Verteilung von $T$ zur Herleitung von Powerfunktionen benutzt werden.


# T-Statistiken
Beispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen
\vspace{2mm}
\footnotesize

Es sei
\begin{equation}
\upsilon \sim N(X\beta,\sigma^2 I_n)
\mbox{ mit }
X := 1_n \in \mathbb{R}^{n\times 1},
\beta := \mu \in \mathbb{R}
\mbox{ und } \sigma^2 > 0.
\end{equation}
das ALM Szenario unabhängiger und identisch normalverteilter Zufallsvariablen und
es seien $c := 1$ und $\beta_0 := \mu_0$. Dann gilt für die T-Statistik
\begin{equation}
T
= \frac{c^T\hat{\beta} - c^T\mu_0}{\sqrt{\hat{\sigma}^2c^T(X^TX)^{-1}c}}
= \frac{1^T\bar{\upsilon}- 1^T\mu_0}{\sqrt{s^2_y 1^T (1_n^T 1_n)^{-1}1}}
= \sqrt{n}\frac{\bar{\upsilon} - \mu_0}{s_\upsilon}
\end{equation}
was der Einstichproben-T-Teststatistik für den Fall $\mu_0 = 0$ entspricht (vgl.
Einheit (12) Hypothesentest in Wahrscheinlichkeitstheorie und Frequentistische
Inferenz und Einheit (9) T-Tests in Allgemeines Lineares Modell). Die hier betrachtete
T-Statistik nimmt hohe Werte für hohe Werte von $\bar{\upsilon}$ (Effekt), kleine Werte
von $s_\upsilon^2$ (Datenvariabilität) und hohe Werte von $n$ (Stichprobenumfang) an.

Eine beliebte Definition in diesem Zusammenhang ist \textit{Cohen's $d$} als \textit{Effektstärkenmaß}.
Es gilt
\begin{equation}
d := \frac{\bar{\upsilon}}{s_\upsilon},
\end{equation}
so dass für $\mu_0 := 0$ gilt, dass
\begin{equation}
T = \sqrt{n}d \mbox{ bzw. } d = \frac{1}{\sqrt{n}} T.
\end{equation}
Cohen's $d$ ist also ein stichprobenumfangunabhängiges Signal-zu-Rauschen Verhältnis.

# T-Statistiken
Simulation (1) Unabhängig und identische normalverteilte Zufallsvariablen

\small
Wahre, aber unbekannte, Hypothesenszenarien $c^T\beta = c^T\beta_0$ und $c^T\beta \neq c^T\beta_0$

\vspace{2mm}
\tiny
\setstretch{1}
```{r}
# Libraries
library(MASS)                                                  # multivariate Normalverteilung

# Modellformulierung
n          = 12                                                # Anzahl von Datenpunkten
p          = 1                                                 # Anzahl von Betaparametern
X          = matrix(c(rep(1,n)), nrow = n)                     # Designmatrix
I_n        = diag(n)                                           # Einheitsmatrix
beta       = c(0,1)                                            # wahre , aber unbekannte , Betaparameter
nscn       = length(beta)                                      # Anzahl wahrer, aber unbekannter, Hypothesenszenarien
sigsqr     = 1                                                 # wahrer, aber unbekannter, Varianzparameter
c          = 1                                                 # Kontrastvektor von Interessse
beta_0     = 0                                                 # Nullhypothesenbetaparameter

# Frequentistische Simulation
nsim       = 1e4                                               # Anzahl Simulationen
delta      = rep(NaN, nscn)                                    # Anzahl Nichtzentralitätsparameter
Tee        = matrix(rep(NaN, nscn*nsim), ncol = nscn)          # T-Teststatistik Realisierungsarray
for(s in 1:nscn){                                              # Hypothesenszenarien
  delta[s]    = ((t(c) %*% beta[s] - t(c) %*% beta_0)/         # Nichtzentralitätsparameter
                sqrt(sigsqr*t(c)%*%solve(t(X)%*%X)%*%c))
  for(i in 1:nsim){                                            # Simulationsiterationen
    y          = mvrnorm(1, X %*% beta[s], sigsqr*I_n)         # y
    beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y              # \hat{\beta}
    eps_hat    = y - X %*% beta_hat                            # \hat{\eps}
    sigsqr_hat = (t(eps_hat) %*% eps_hat)/(n-p)                # \hat{\sigma}^2
    Tee[i,s]   = ((t(c) %*% beta_hat - t(c) %*% beta_0)/       # T
                  sqrt(sigsqr_hat*t(c)%*%solve(t(X)%*%X)%*%c))
  }
}
```

# T-Statistiken
Simulation (1) Unabhängig und identische normalverteilte Zufallsvariablen

\footnotesize
Wahre, aber unbekannte, Hypothesenszenarien $c^T\beta = c^T\beta_0$ und $c^T\beta \neq c^T\beta_0$

\vspace{8mm}

```{r, eval = F, echo = F}

# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# T-Teststatistik Ergebnisraum
xlims  = c(-5,12)
t_min  = xlims[1]
t_max  = xlims[2]
t_res  = 1e3
t      = seq(t_min, t_max, len = t_res)
lab    = c(TeX("$\\c^T beta = \\c^T beta_0$"), TeX("$\\c^T beta \\neq \\c^T beta_0$") )

# T-Teststatistiken
for(s in 1:nscn){
  p_t    = dt(t,n-p, delta[s])
  hist(
  Tee[,s],
  breaks = 50,
  col    = "gray90",
  prob   = TRUE,
  xlab   = TeX("$T$"),
  ylab   = "",
  xlim   = xlims,
  ylim   = c(0,.4),
  main   = lab[s])
  lines(
  t,
  p_t,
  type  = "l",
  lwd   = 2,
  col   = "darkorange")
}

# Speichern
dev.copy2pdf(
file        = file.path(fdir, "alm_7_T_Teststatistik_1.pdf"),
width       = 8,
height      = 4)
```


```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("7_Abbildungen/alm_7_T_Teststatistik_1.pdf")
```

# T-Statistiken
Simulation (1) Einfache lineare Regression

\small
Wahre, aber unbekannte, Hypothesenszenarien $c^T\beta = c^T\beta_0$ und $c^T\beta \neq c^T\beta_0$

\vspace{2mm}
\tiny
\setstretch{1}
```{r}
# Modellformulierung
library(MASS)                                                  # multivariate Normalverteilung
n          = 10                                                # Anzahl von Datenpunkten
p          = 2                                                 # Anzahl von Betaparametern
x          = 1:n                                               # Prädiktorwerte
X          = matrix(c(rep(1,n),x), ncol = p)                   # Designmatrix
I_n        = diag(n)                                           # Einheitsmatrix
beta       = matrix(c(1,0,1,1), nrow = 2)                      # wahre , aber unbekannte , Betaparameter
nscn       = ncol(beta)                                        # Anzahl wahrer, aber unbekannter, Hypothesenszenarien
sigsqr     = 1                                                 # wahrer, aber unbekannter, Varianzparameter
c          = matrix(c(0,1), nrow = 2)                          # Kontrastvektor von Interessse
beta_0     = matrix(c(0,0), nrow = 2)                          # Nullhypothesenbetaparameter

# Frequentistische Simulation
nsim       = 1e4                                               # Anzahl Simulationen
delta      = rep(NaN, nscn)                                    # Anzahl Nichtzentralitätsparameter
Tee        = matrix(rep(NaN, nscn*nsim), ncol = nscn)          # T-Teststatistik Realisierungsarray
for(s in 1:nscn){                                              # Hypothesenszenarien
  delta[s]    = ((t(c) %*% beta[,s] - t(c) %*% beta_0)/        # Nichtzentralitätsparameter
                sqrt(sigsqr*t(c)%*%solve(t(X)%*%X)%*%c))
  for(i in 1:nsim){                                            # Simulationsiterationen
    y          = mvrnorm(1, X %*% beta[,s], sigsqr*I_n)        # y
    beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y              # \hat{\beta}
    eps_hat    = y - X %*% beta_hat                            # \hat{\eps}
    sigsqr_hat = (t(eps_hat) %*% eps_hat)/(n-p)                # \hat{\sigma}^2
    Tee[i,s]   = ((t(c) %*% beta_hat - t(c) %*% beta_0)/       # T
                  sqrt(sigsqr_hat*t(c)%*%solve(t(X)%*%X)%*%c))
  }
}
```

# T-Statistiken
Simulation (1) Einfache lineare Regression

\small
Wahre, aber unbekannte, Hypothesenszenarien $c^T\beta = c^T\beta_0$ und $c^T\beta \neq c^T\beta_0$

\vspace{8mm}

```{r, eval = F, echo = F}
# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# T-Teststatistik Ergebnisraum
xlims  = c(-5,20)
t_min  = xlims[1]
t_max  = xlims[2]
t_res  = 1e3
t      = seq(t_min, t_max, len = t_res)
lab    = c(TeX("$\\c^T beta = \\c^T beta_0$"), TeX("$\\c^T beta \\neq \\c^T beta_0$") )

# T-Teststatistiken
for(s in 1:nscn){
  p_t    = dt(t,n-p, delta[s])
  hist(
  Tee[,s],
  breaks = 50,
  col    = "gray90",
  prob   = TRUE,
  xlab   = TeX("$T$"),
  ylab   = "",
  xlim   = xlims,
  ylim   = c(0,.4),
  main   = lab[s])
  lines(
  t,
  p_t,
  type  = "l",
  lwd   = 2,
  col   = "darkorange")
}

# Speichern
dev.copy2pdf(
file        = file.path(fdir, "alm_7_T_Teststatistik_2.pdf"),
width       = 8,
height      = 4)
```

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("7_Abbildungen/alm_7_T_Teststatistik_2.pdf")
```


# 
\vfill
\large
\setstretch{3}
T-Zufallsvariablen

T-Statistiken

**Konfidenzintervalle**

Selbstkontrollfragen
\vfill


# Konfidenzintervalle
\footnotesize
\begin{theorem}[Konfidenzintervalle für Betaparameterkomponenten]
\justifying
\normalfont
Es sei
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
das ALM,  $\hat{\beta}$ und $\hat{\sigma}^2$ seien die Betaparameter- und Varianzparameterschätzer,
respektive und und für ein $\delta \in ]0,1[$ sei 
\begin{equation}
t_\delta := \Psi^{-1}\left(\frac{1+\delta}{2}; n - p \right).
\end{equation}
Schließlich sei für $j = 1,...p$
\begin{equation}
\lambda_j := \left(\left(X^TX \right)^{-1}\right)_{jj}
\mbox{ das $j$te Diagonalelement von } \left(X^TX \right)^{-1}. 
\end{equation}
Dann ist für $j = 1,...p$
\begin{equation}
\kappa_j := \left[\hat{\beta}_j - \hat{\sigma}\sqrt{\lambda_j}t_{\delta},\hat{\beta}_j + \hat{\sigma}\sqrt{\lambda_j}t_{\delta}\right]
\end{equation}
ein $\delta$-Konfidenzintervall für die $j$te Komponente $\beta_j$ des Betaparameters $\beta = (\beta_1,...,\beta_p)^T$.
\end{theorem}

Bemerkungen

* Intuitiv gilt im Vergleich zum  Konfidenzintervall für den Erwartungswertparameter bei Normalverteilung
\begin{equation}
\hat{\beta}_j \approx \bar{\upsilon}, \hat{\sigma} \approx S, \sqrt{\lambda_j} \approx \sqrt{n^{-1}} \mbox{ und } t_\delta = t_\delta,
\end{equation}
vgl. (11) Konfidenzintervalle in Wahrscheinlichkeitstheorie und Frequentistische Inferenz.

# Konfidenzintervalle
\footnotesize
\underline{Beweis}

Wir müssen zeigen, dass 
\begin{equation}
\mathbb{P}(\kappa_j \ni \beta_j) = \delta.
\end{equation}
Dazu halten wir zunächst fest, dass für alle $j = 1,...,p$ bei Wahl von $\beta_0 = \beta$ 
und $c := e_j$  nach dem Theorem zur T-Statistik für $T \sim t(\delta,n-p)$ gilt, dass
\begin{align}
\begin{split}
T 
= \frac{e_j^T\hat{\beta} - e_j^T\beta}{\sqrt{\hat{\sigma}^2e_j^T\left(X^TX\right)^{-1}e_j}} 
= \frac{\hat{\beta}_j - \beta_j}{\sqrt{\hat{\sigma}^2 \left(\left(X^TX \right)^{-1}\right)_{jj}}}
= \frac{\hat{\beta}_j - \beta_j}{\hat{\sigma} \sqrt{\lambda_j}}
=: T_j.
\end{split}
\end{align}
und
\begin{align}
\begin{split}
\delta
= \frac{e_j\beta - e_j^T\beta}{\sqrt{\hat{\sigma}^2e_j^T\left(X^TX\right)^{-1}e_j}} 
= 0
\end{split}
\end{align}
Damit gilt dann auch sofort, dass $T_j \sim t(n-p)$. Weiterhin erinnern erinnern 
wir daran (vgl. (11) Konfidenzintervalle in Wahrscheinlichkeitstheorie und 
Frequentistischer Inferenz), dass per Definition von $t_\delta$ gilt, dass
\begin{equation}
\mathbb{P}\left(-t_\delta \le T_j \le t_\delta \right)
\end{equation}

# Konfidenzintervalle
\footnotesize
\underline{Beweis (fortgeführt)}

Aus der Definition eines $\delta$-Konfidenzintervalls folgt dann
\begin{align}
\begin{split}
\delta 
& = \mathbb{P}\left(-t_\delta \le T_j \le t_\delta \right) \\
& = \mathbb{P}\left(-t_\delta \le \frac{\hat{\beta}_j - \beta_j}{\hat{\sigma}\sqrt{\lambda_j}} \le t_\delta \right) \\
& = \mathbb{P}\left(-t_\delta\hat{\sigma}\sqrt{\lambda_j} \le \hat{\beta}_j - \beta_j \le t_\delta\hat{\sigma}\sqrt{\lambda_j} \right) \\
& = \mathbb{P}\left(-\hat{\beta}_j -t_\delta\hat{\sigma}\sqrt{\lambda_j} \le - \beta_j \le -\hat{\beta}_j + t_\delta\hat{\sigma}\sqrt{\lambda_j} \right) \\
& = \mathbb{P}\left(\hat{\beta}_j + t_\delta\hat{\sigma}\sqrt{\lambda_j} \ge \beta_j \ge \hat{\beta}_j - t_\delta\hat{\sigma}\sqrt{\lambda_j} \right) \\
& = \mathbb{P}\left(\hat{\beta}_j - t_\delta\hat{\sigma}\sqrt{\lambda_j}  \le \beta_j \le \hat{\beta}_j + t_\delta\hat{\sigma}\sqrt{\lambda_j} \right) \\
& = \mathbb{P}\left(\left[\hat{\beta}_j - \hat{\sigma}\sqrt{\lambda_j}t_{\delta},\hat{\beta}_j + \hat{\sigma}\sqrt{\lambda_j}t_{\delta}\right]  \ni \beta_j \right) \\
& = \mathbb{P}(\kappa_j \ni \beta_j) 
\end{split}
\end{align}
und damit ist alles gezeigt.

# Konfidenzintervalle

Konfidenzintervall bei unabhängigen und identische normalverteilten Zufallsvariablen

\footnotesize
Wir betrachten die ALM Form des Szenarios unabhängig und identisch normalverteilter Zufallsvariablen

\begin{equation}
\upsilon \sim N(X\beta,\sigma^2I_n) \mbox{ mit }  X := 1_{n} \in \mathbb{R}^n, \beta := \mu \in \mathbb{R},\sigma^2 > 0
\end{equation}
Dann gelten, wie bereits gesehen
\begin{equation}
\hat{\beta} = \frac{1}{n}\sum_{i=1}^n \upsilon_i =: \bar{\upsilon},
\hat{\sigma}^2 = \frac{1}{n-1}\sum_{i=1}^n(\upsilon_i-\bar{\upsilon})^2 =: s^2 \mbox{ und }
\lambda_1 = \left(1_n^T1_n\right)^{-1} = \frac{1}{n}
\end{equation}
Nach dem Theorem zu Konfidenzintervallen für Betaparameterkomponenten gilt dann,
dass
\begin{equation}
\kappa := \left[\bar{\upsilon} - \frac{s}{\sqrt{n}}t_{\delta}, \bar{\upsilon} + \frac{s}{\sqrt{n}}t_{\delta}\right]
\end{equation}
ein $\delta$-Konfidenzintervall für $\beta$ ist und dieses ist offenbar identisch
mit dem $\delta$-Konfidenzintervall für den Erwartungsparameter der Normalverteilung,
welches wir in (9) Konfidenzintervalle in Wahrscheinlichkeitstheorie und Frequentistische Inferenz
eingeführt haben.

# Konfidenzintervalle
Simulation von Konfidenzintervallen bei einfacher linearer Regression

\vspace{1mm}
\setstretch{1.1}
\tiny
```{r}
# Modellformulierung
library(MASS)                                                       # multivariate Normalverteilung
set.seed(0)                                                         # random number generator seed
ns         = 1e2                                                    # Anzahl Simulationen
n          = 10                                                     # Anzahl von Datenpunkten
p          = 2                                                      # Anzahl von Betaparametern
x          = 1:n                                                    # Prädiktorwerte
X          = matrix(c(rep(1,n),x), ncol = p)                        # Designmatrix
I_n        = diag(n)                                                # Einheitsmatrix
beta       = matrix(c(1,2), nrow = 2)                               # wahre, aber unbekannte, Betaparameter
sigsqr     = 1                                                      # wahrer, aber unbekannter, Varianzparameter
delta      = 0.95                                                   # Konfidenzbedingung  
t_delta    = qt((1+delta)/2,n-1)                                    # \Psi^{-1}((1+\delta)/2,n-1)
lambda     = diag(solve(t(X) %*% X))                                # \lambda_j Werte

# Simulation
kappa      = array(rep(NaN, ns*p*p), dim=c(ns,2,2))                 # Konfidenzintervallarray  
beta_hat   = matrix(rep(NaN,p*ns), nrow = p)                        # Betaparameterschätzer
for(i in 1:ns){                                                     # Iteration über Realisierungen
  y              = mvrnorm(1, X %*% beta, sigsqr*I_n)               # Datenrealisierung
  beta_hat[,i]   = solve(t(X) %*% X) %*% t(X) %*% y                 # \hat{\beta}
  eps_hat        = y - X %*% beta_hat[,i]                           # \hat{\varepsilon}
  sigsqr_hat     = (t(eps_hat) %*% eps_hat)/(n-p)                   # \hat{\sigma}^2
  for(j in 1:p){                                                    # Iteration über Betaarraykomponenten
    kappa[i,1,j] = beta_hat[j,i]-sqrt(sigsqr_hat*lambda[j])*t_delta # untere KI Grenze
    kappa[i,2,j] = beta_hat[j,i]+sqrt(sigsqr_hat*lambda[j])*t_delta # obere  KI Grenze
  }
}
```

# Konfidenzintervalle
Simulation von Konfidenzintervallen bei einfacher linearer Regression

```{r, eval = F, echo = F}
# Visualisierung
graphics.off()
dev.new()
library(latex2exp)
labels      = c(TeX("Offsetparameter $\\beta_1 = 1,\\,\\, \\sigma^2 = 1, \\, n = 10,\\, \\delta = 0.95"),
                TeX("Slopeparameter  $\\beta_2 = 2,\\,\\, \\sigma^2 = 1, \\, n = 10,\\, \\delta = 0.95"))
ylimits     = list(c(-3,5), c(1,3.5))
mp          = c(4,3)
par(
family      = "sans",
mfcol       = c(2,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(3,2,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1)
for(j in 1:p){
  P_idx      = rep(NaN,ns)                                                       # Nicht überdeckende KIs für Visualisierung
  P_idx[beta[j] < kappa[,1,j] | beta[j] > kappa[,2,j]]  = mp[j]                     # Markerpositions
  plot(
  1:ns,
  beta_hat[j,],
  type    = "p",
  ylim    = ylimits[[j]],
  xlim    = c(0,102),
  xlab    = "Simulationen",
  ylab    = "",
  pch     =  19,
  cex     =  .5,
  main    = labels[j])
  arrows(
  x0      = 1:ns,
  y0      = kappa[,1,j],
  x1      = 1:ns,
  y1      = kappa[,2,j],
  code    = 3,
  angle   = 90,
  length  = 0.01,
  lwd     = .7)
  lines(
  1:ns,
  rep(beta[j],ns),
  col      = "gray80",
  lty      = 1)
  lines(
  1:ns,
  P_idx,
  type    = "p",
  pch     = 13,
  col     = "darkorange")
}

# PDF Speicherung
dev.copy2pdf(
file     = file.path("7_Abbildungen/alm_7_elr_konfidenzintervalle.pdf"),
width    = 9,
height   = 7)
```

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("7_Abbildungen/alm_7_elr_konfidenzintervalle.pdf")
```


# 
\vfill
\large
\setstretch{3}
T-Verteilungen

T-Statistiken

Konfidenzintervalle

**Selbstkontrollfragen**
\vfill

# Selbstkontrollfragen
\footnotesize
\setstretch{2.2}
1. Skizzieren Sie die WDFen von $t$-Zufallsvariablen mit 2, 10 und 30 Freiheitsgraden.
1. Skizzieren Sie die WDFen von nichtzentralen $t$-Zufallsvariablen mit Nichtzentralitätsparametern 0,5 und 15.
1. Geben Sie die Definition der T-Statistik wieder.
1. Erläutern Sie für die T-Statistik die Bedeutung der Wahl von $c \in \mathbb{R}^p$.
1. Erläutern Sie für die T-Statistik die Bedeutung der Wahl von $\beta_0 \in \mathbb{R}^p$.
1. Wann und warum kann die T-Statistik als Signal-zu-Rauschen Verhältnis interpretiert werden?
1. Geben Sie das Theorem zur T-Statistik wieder.
1. Geben Sie die Form der T-Statistik im Szenario von $n$ u.i.n.v. Zufallsvariablen wieder.
1. Erläutern Sie den Zusammenhang der T-Statistik und Cohen's $d$.
1. Geben Sie das Theorem zu Konfidenintervallen für Betaparameterkomponenten wieder.

# Referenzen
\footnotesize
\setstretch{2.2}