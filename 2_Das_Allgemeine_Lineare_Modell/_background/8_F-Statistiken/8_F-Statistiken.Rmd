---
fontsize: 8pt
bibliography: 8_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 8_header.tex
---


```{r, include = F}
source("8_R_common.R")
fdir        = file.path(getwd(), "8_Abbildungen")                                 
```


#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("8_Abbildungen/alm_8_otto.png")
```

\vspace{2mm}

\huge
Allgemeines Lineares Modell
\vspace{6mm}

\large
BSc Psychologie SoSe 2023

\vspace{6mm}
\normalsize
Prof. Dr. Dirk Ostwald

#  {.plain}
\center
\huge
\vfill
\noindent (8) F-Statistiken
\vfill

#
\large

Naturwissenschaft \vspace{7mm}

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("8_Abbildungen/alm_8_wissenschaft.pdf")
```

#
\vspace{1mm}
\normalsize
Modellformulierung
\vspace{1mm}
\small
\begin{equation}
\upsilon = X\beta + \varepsilon, \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
\vspace{5mm}

\normalsize
Modellschätzung
\small
\begin{equation}
\hat{\beta} = (X^TX)^{-1} X^T\upsilon,  \hat{\sigma}^2 = \frac{(\upsilon-X\hat{\beta})^T(\upsilon-X\hat{\beta})}{n-p}
\end{equation}
\vspace{4mm}

\normalsize
Modellevaluation
\small
\begin{equation}
T = \frac{c^T\hat{\beta} - c^T\beta_0}{\sqrt{\hat{\sigma}^2c^T(X^TX)^{-1}c}},
F = \frac{(\hat{\varepsilon}_0^T\hat{\varepsilon}_0 - \hat{\varepsilon}^T\hat{\varepsilon})/p_1}{\hat{\varepsilon}^T\hat{\varepsilon}/(n-p)}
\end{equation}

#
Standardprobleme Frequentistischer Inferenz

\small
\vspace{2mm}
\noindent (1) Parameterschätzung

Ziel der Parameterschätzung ist es, einen möglichst guten Tipp für wahre, aber unbekannte,
Parameterwerte oder Funktionen dieser abzugeben, typischerweise mithilfe von Daten.

\vspace{2mm}
\noindent (2) Konfidenzintervalle

Ziel der Bestimmung von Konfidenzintervallen ist es, basierend auf der angenommenen
Verteilung der Daten eine quantitative Aussage über die mit Schätzwerten assoziierte
Unsicherheit zu treffen.

\vspace{2mm}
\noindent (3) Hypothesentests

Ziel des Hypothesentestens ist es, basierend auf der angenommenen
Verteilung der Daten in einer möglichst zuverlässigen Form zu entscheiden, ob ein
wahrer, aber unbekannter Parameterwert in einer von zwei sich gegenseitig
ausschließenden Untermengen des Parameterraumes liegt.

#
\center
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("8_Abbildungen/alm_8_frequentistische_inferenz.pdf")
```
\center
\footnotesize
$\theta := (\beta,\sigma^2)$,
$\Theta := \mathbb{R}^p \times \mathbb{R}_{>0}$
$\mathbb{P}_\theta(\upsilon) := \mathbb{P}_{\beta,\sigma^2}(\upsilon)$ mit WDF $p_{\beta,\sigma^2}(y) := N(y;X\beta,\sigma^2I_n)$

#
\small
Standardannahmen Frequentistischer Inferenz

\footnotesize
\setstretch{1.2}
Gegeben sei das Allgemeine Lineare Modell. Es wird angenommen, dass ein
vorliegender Datensatz eine der möglichen Realisierungen der Daten des Modells ist.
Aus Frequentistischer Sicht kann man unendlich oft Datensätze basierend auf einem
Modell generieren und zu jedem Datensatz Schätzer oder Statistiken auswerten, z.B.
den Betaparameterschätzer
\vspace{1mm}

\begin{itemize}
\item[] Datensatz (1) : $y^{(1)} = \left(y_1^{(1)}, y_2^{(1)}, ...,y_n^{(1)}\right)^T$ 	mit $\hat{\beta}^{(1)} = (X^TX)^{-1}X^Ty^{(1)}$
\item[] Datensatz (2) : $y^{(2)} = \left(y_1^{(2)}, y_2^{(2)}, ...,y_n^{(2)}\right)^T$ 	mit $\hat{\beta}^{(2)} = (X^TX)^{-1}X^Ty^{(2)}$
\item[] Datensatz (3) : $y^{(3)} = \left(y_1^{(3)}, y_2^{(3)}, ...,y_n^{(3)}\right)^T$ 	mit $\hat{\beta}^{(3)} = (X^TX)^{-1}X^Ty^{(3)}$
\item[] Datensatz (4) : $y^{(4)} = \left(y_1^{(4)}, y_2^{(4)}, ...,y_n^{(4)}\right)^T$ 	mit $\hat{\beta}^{(4)} = (X^TX)^{-1}X^Ty^{(4)}$
\item[] Datensatz (5) : $y^{(5)} = ...$
\end{itemize}

\vspace{1mm}
Um die Qualität statistischer Methoden zu beurteilen betrachtet die Frequentistische
Statistik die Wahrscheinlichkeitsverteilungen von Schätzern und Statistiken
unter Annahme der Datenverteilung. Was zum Beispiel ist die Verteilung von
$\hat{\beta}^{(1)}$, $\hat{\beta}^{(2)}$, $\hat{\beta}^{(3)}$, $\hat{\beta}^{(4)}$, ... also die
Verteilung der Zufallsvariable $\hat{\beta} := (X^TX)^{-1}X^T\upsilon$? Wenn eine statistische
Methode im Sinne der Frequentistischen Standardannahmen "gut" ist, dann heißt das
also, dass sie bei häufiger Anwendung "im Mittel gut" ist. Im Einzelfall, also
im Normalfall nur eines vorliegenden Datensatzes, kann sie auch "schlecht" sein.

#
\small
Überblick

* \justifying Wir führen F-Statistiken hier vor dem Hintergrund Likelihood-Quotienten-basierter 
Modellvergleiche ein. Die (maximierte oder marginale) Likelihood eines Datensatzes unter einem
gegebenen probabilistischen Modell als Modellvergleichskriterium heranzuziehen ist
ein weit verbreitetes Verfahren in der probabilistischen Datenanalyse.

* \justifying Im Gegensatz zu T-Statistiken kann das Ziel der Berechnung von F-Statistiken damit
insbesondere sein, nicht nur Linearkombinationen von Betaparameterschätzwerten 
probabilistisch zu evaluieren, sondern die Modellanpassung an einen Datensatz 
insgesamt zu evaluieren.

* \justifying Die Modellvergleichskapazität von F-Statistiken ist allerdings etwas beschränkt,
da sich die F-Statistik nur auf ALMs und insbesondere geschachtelte ALMs bezieht, 
in denen ein Modell Bestandteil eines anderen Modells ist.

* \justifying F-Statistiken bilden üblicherweise die Grundlage für Hypothesentests 
im Rahmen varianzanalytischer Verfahren (vgl. (10) Einfaktorielle Varianzanalyse, (11) Zweifaktorielle 
Varianzanalyse und (13) Kovarianzanalyse). Der Einsatz von F-Statistiken ist aber 
*perse* nicht auf Varianzanalysen beschränkt, sondern kann auch bei parametrischen ALM Designs angebracht sein.


#
\vfill
\large
\setstretch{3}
F-Zufallsvariablen

Likelihood-Quotienten

Definition und Verteilung

Selbstkontrollfragen
\vfill


#
\vfill
\large
\setstretch{3}
**F-Zufallsvariablen**

Likelihood-Quotienten

Definition und Verteilung

Selbstkontrollfragen
\vfill

# F-Zufallsvariablen
\footnotesize
\begin{definition}[$f$-Zufallsvariable]
\justifying
$\xi$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}_{>0}$ und Wahrscheinlichkeitsdichtefunktion
\begin{equation}
p_\xi : \mathbb{R} \to \mathbb{R}_{>0}, x \mapsto p_\xi(x)
:= \nu_1^{\frac{\nu_1}{2}}\nu_2^{\frac{\nu_2}{2}}
   \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right)}{\Gamma\left(\frac{\nu_1}{2}\right)\Gamma\left(\frac{\nu_2}{2}\right)}
   \frac{x^{\frac{\nu_1}{2}-1}}{\left(\nu_1 x  + \nu_2 \right)^{\frac{\nu_1+\nu_2}{2}}},
\end{equation}
wobei $\Gamma$ die Gammafunktion bezeichne. Dann sagen wir, dass $\xi$ einer
$f$-Verteilung mit Freiheitsgradparametern $\nu_1$ und $\nu_2$ unterliegt und nennen $\xi$ eine
$f$-Zufallsvariable mit Freiheitsgradparametern $\nu_1$ und $\nu_2$. Wir kürzen dies
mit $\xi \sim f(\nu_1,\nu_2)$ ab. Die Wahrscheinlichkeitsdichtefunktion (WDF) einer
$f$-Zufallsvariable bezeichnen wir mit $f(x;\nu_1,\nu_2)$, die kumulative Verteilungsfunktion (KVF)
einer $f$-Zufallsvariable bezeichnen wir mit $\varphi(x;\nu_1,\nu_2)$, und die inverse
kumulative Verteilungsfunktion einer $f$-Zufallsvariable bezeichnen wir mit $\varphi^{-1}(x;\nu_1,\nu_2)$.
\end{definition}

Bemerkungen

* $f$-Zufallsvariablen sind nach Ronald A. Fisher benannt.
* George W. Snedecor hat die KVF der $f$-Verteilung wohl 1934 basierend auf Arbeiten von Fisher tabuliert.

# F-Zufallsvariablen
\vfill
```{r, echo = F, eval = F}
library(latex2exp)
dev.new()
par(
family     = "sans",
pty        = "m",
bty        = "l",
lwd        = 1,
las        = 1,
mgp        = c(2,1,0),
xaxs         = "i",
yaxs         = "i",
font.main  = 1,
cex.main   = 1.2)

# x space
x_min   = 0
x_max   = 6
x_res   = 1e3
x       = seq(x_min, x_max, len = x_res)

# parameters of interest
nu_1    = c( 2,  2,  3,  3,  4,  4)
nu_2    = c(13, 26, 13, 26, 13, 26)

# plotting
matplot(x,
matrix(
c(
df(x,nu_1[1],nu_2[1]),
df(x,nu_1[2],nu_2[2]),
df(x,nu_1[3],nu_2[3]),
df(x,nu_1[4],nu_2[4]),
df(x,nu_1[5],nu_2[5]),
df(x,nu_1[6],nu_2[6])),
ncol = 6),
type         = "l",
lty          = c(1,2,1,2,1,2),
lwd          = c(2,1,2,1,2,1),
col          = c("gray20", "gray20", "gray50", "gray50", "gray80", "gray80"),
ylim         = c(0,1),
xlim         = c(x_min,x_max),
ylab         = " ",
xlab         = "x",
main         = TeX("$f(x;\\nu_1,\\nu_2)$"))
legend(
4.00,
1.00,
c(
TeX("$\\nu_1 = 2,  \\nu_2 = 13 "),
TeX("$\\nu_1 = 2,  \\nu_2 = 26"),
TeX("$\\nu_1 = 3,  \\nu_2 = 13 "),
TeX("$\\nu_1 = 3,  \\nu_2 = 26"),
TeX("$\\nu_1 = 4,  \\nu_2 = 13 "),
TeX("$\\nu_1 = 4,  \\nu_2 = 26")),
lty         = c(1,2,1,2,1,2),
lwd         = c(2,1,2,1,2,1),
col         = c("gray20", "gray20", "gray50", "gray50", "gray80",  "gray80"),
bty         = "n",
cex         = .8,
y.intersp   = 1.4)
dev.copy2pdf(
file   = file.path(getwd(), "8_Abbildungen", "alm_8_f_wdf.pdf"),
width  = 6,
height = 4)
```
\small
Wahrscheinlichkeitsdichtefunktionen von $f$-Verteilungen
\vspace{1mm}
```{r, echo = FALSE, out.width="80%"}
knitr::include_graphics("8_Abbildungen/alm_8_f_wdf.pdf")
```

# F-Zufallsvariablen
\small
\begin{theorem}[$F$-Transformation]
\justifying
\normalfont
$\zeta_1 \sim \chi^2(\nu_1)$ und $\zeta_2 \sim \chi^2(\nu_2)$ seien zwei unabhängige
$\chi^2$-Zufallfsvariablen mit $\nu_1$ und $\nu_2$ Freiheitsgraden, respektive.
Dann ist die Zufallsvariable
\begin{equation}
\xi := \frac{\zeta_1/\nu_1}{\zeta_2/\nu_2}
\end{equation}
eine $f$-verteilte Zufallsvariable mit $\nu_1,\nu_2$ Freiheitsgraden, es gilt also $\xi \sim f(\nu_1,\nu_2)$.
\end{theorem}

\footnotesize
Bemerkungen

* Wir verzichten auf einen Beweis.
* Das Theorem kann bewiesen werden, in dem man zunächst ein Transformationstheorem
für Quotienten von Zufallsvariablen mithilfe des multivariaten Transformationstheorems
und Marginalisierung herleitet und dieses Theorem dann auf die WDF von $\chi^2$-verteilten
ZVen anwendet. Dabei ist die Regel zur Integration durch Substitution von zentraler Bedeutung.

# F-Zufallsvariablen
\footnotesize
\begin{definition}[Nichtzentrale $f$-Zufallsvariable]
\justifying
$\xi$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}_{>0}$ und Wahrscheinlichkeitsdichtefunktion
\begin{multline}
p_\xi : \mathbb{R} \to \mathbb{R}_{>0}, x \mapsto \\
p_\xi(x)
:= \sum_{k=0}^\infty \frac{e^{-\delta/2}(\delta/2)^k}{\frac{\Gamma(\nu_2/2)\Gamma(\nu_1/2 + k)}{\Gamma(\nu_2/2 + \nu_1/2 + k)}k!}
    \left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2 + k}
    \left(\frac{\nu_2}{\nu_2+\nu_1x}\right)^{(\nu_1+\nu_2)/2 + k}
    x^{\nu_1/2 - 1 + k}
\end{multline}
wobei $\Gamma$ die Gammafunktion bezeichne. Dann sagen wir, dass $\xi$ einer
nichtzentralen $f$-Verteilung mit Nichtzentralitätsparameter $\delta$ und
Freiheitsgradparametern $\nu_1$ und $\nu_2$ unterliegt und nennen $\xi$ eine nichtzentrale
$f$-Zufallsvariable mit Nichtzentralitätsparameter $\delta$ und Freiheitsgradparametern
$\nu_1$ und $\nu_2$. Wir kürzen dies mit $\xi \sim f(\delta,\nu_1,\nu_2)$ ab.
Die Wahrscheinlichkeitsdichtefunktion (WDF) einer $f$-Zufallsvariable
bezeichnen wir mit $f(x;\delta,\nu_1,\nu_2)$, die kumulative Verteilungsfunktion (KVF)
einer nichtzentralen $f$-Zufallsvariable bezeichnen
wir mit $\varphi(x;\delta,\nu_1,\nu_2)$, und die inverse kumulative Verteilungsfunktion
einer nichtzentralen $f$-Zufallsvariable
bezeichnen wir mit $\varphi^{-1}(x;\delta,\nu_1,\nu_2)$.
\end{definition}
Bemerkungen

* Es gilt $f(0,\nu_1,\nu_2) = f(\nu_1,\nu_2)$.

# F-Zufallsvariablen
\vfill
```{r, echo = F, eval = F}
library(latex2exp)
dev.new()
par(
family     = "sans",
pty        = "m",
bty        = "l",
lwd        = 1,
las        = 1,
mgp        = c(2,1,0),
xaxs         = "i",
yaxs         = "i",
font.main  = 1,
cex.main   = 1.2)

# x space
x_min   = 0
x_max   = 6
x_res   = 1e3
x       = seq(x_min, x_max, len = x_res)

# parameters of interest
delta   = c(  0,  0,  4,  4,  8,  8)
nu_1    = c(  2,  2,  2,  2,  2,  2)
nu_2     = c(13, 26, 13, 26, 13, 26)

# plotting
matplot(x,
matrix(
c(
df(x,nu_1[1],nu_2[1],delta[1]),
df(x,nu_1[2],nu_2[2],delta[2]),
df(x,nu_1[3],nu_2[3],delta[3]),
df(x,nu_1[4],nu_2[4],delta[4]),
df(x,nu_1[5],nu_2[5],delta[5]),
df(x,nu_1[6],nu_2[6],delta[6])),
ncol = 6),
type         = "l",
lty          = c(1,2,1,2,1,2),
lwd          = c(2,1,2,1,2,1),
col          = c("gray20", "gray20", "gray50", "gray50", "gray80",  "gray80"),
ylim         = c(0,1),
xlim         = c(x_min,x_max),
ylab         = " ",
xlab         = "x",
main         = TeX("$f(x; \\delta, \\nu_1, \\nu_2)$"))
legend(
4.00,
1.00,
c(
TeX("$\\delta = 0, \\nu_1 = 2,  \\nu_2 = 13"),
TeX("$\\delta = 0, \\nu_1 = 2,  \\nu_2 = 26"),
TeX("$\\delta = 4, \\nu_1 = 2,  \\nu_2 = 13"),
TeX("$\\delta = 4, \\nu_1 = 2,  \\nu_2 = 26"),
TeX("$\\delta = 8, \\nu_1 = 2,  \\nu_2 = 13 "),
TeX("$\\delta = 8, \\nu_1 = 2,  \\nu_2 = 26")),
lty         = c(1,2,1,2,1,2),
lwd         = c(2,1,2,1,2,1),
col         = c("gray20", "gray20", "gray50", "gray50", "gray80",  "gray80"),
bty         = "n",
cex         = .8,
y.intersp   = 1.4)
dev.copy2pdf(
file   = file.path("./8_Abbildungen/alm_8_f_nichtzentral_wdf.pdf"),
width  = 6,
height = 4)
```

\small
WDFen von nichtzentralen $f$-Verteilungen
\vspace{1mm}
```{r, echo = FALSE, out.width="80%"}
knitr::include_graphics("8_Abbildungen/alm_8_f_nichtzentral_wdf.pdf")
```

# F-Zufallsvariablen
\small
\begin{theorem}[Nichtzentrale $F$-Transformation]
\justifying
\normalfont
$\zeta_1 \sim \chi^2(\delta,\nu_1)$ und $\zeta_2 \sim \chi^2(\nu_2)$ seien eine nichtzentrale $\chi^2$-
Zufallsvariable und eine  $\chi^2$-Zufallsvariable mit $\nu_1$ und $\nu_2$ Freiheitsgraden,
respektive und $\zeta_1$ und $\zeta_2$ seien unabhängig. Dann ist die Zufallsvariable
\begin{equation}
\xi := \frac{\zeta_1/\nu_1}{\zeta_2/\nu_2}
\end{equation}
eine nichtzentral $f$-verteilte Zufallsvariable mit Nichtzentralitätsparameter
$\delta$  und $\nu_1,\nu_2$ Freiheitsgraden, es gilt also $\xi \sim f(\delta, \nu_1,\nu_2)$.
\end{theorem}

\footnotesize
Bemerkungen

* Wir verzichten auf einen Beweis.

# 
\vfill
\large
\setstretch{3}
F-Zufallsvariablen

**Likelihood-Quotienten-Statistiken**

Definition und Verteilung

Selbstkontrollfragen
\vfill

# Likelihood-Quotienten-Statistiken
\footnotesize
\begin{definition}[Likelihood-Quotienten-Statistik]
\justifying
Gegeben seien zwei parametrische statistische Modelle 
\begin{equation}
\mathcal{M}_0 := \left(\mathcal{Y},\mathcal{A}, \left\{\mathbb{P}^0_{\theta_0}|\theta_0 \in \Theta_0\right\}\right)
\mbox{ und }
\mathcal{M}_1 := \left(\mathcal{Y},\mathcal{A}, \left\{\mathbb{P}^1_{\theta_1}|\theta_1 \in \Theta_1\right\}\right)
\end{equation}
mit identischem Datenraum, identischer $\sigma$-Algebra und potentiell distinkten 
Wahrscheinlichkeitsmaßmengen und Parameterräumen. Sei weiterhin $\upsilon$ 
ein Zufallsvektor mit Datenraum $\mathcal{Y}$. Seien schließlich $L_0^\upsilon$ 
und $L_1^\upsilon$ die Likelihood-Funktionen von $\mathcal{M}_0$ und $\mathcal{M}_1$, 
respektive, wobei das Superskript $^\upsilon$ jeweils an die Datenabhängigkeit 
der Likelihood Funktion erinnern soll. Dann wird 
\begin{equation}
\Lambda := \frac{\max_{\theta_0 \in \Theta_0}L^\upsilon_0(\theta_0)}{\max_{\theta_1 \in \Theta_1}L^\upsilon_1(\theta_1)}, 
\end{equation}
\textit{Likelihood-Quotienten-Statistik} genannt.
\end{definition}

# Likelihood-Quotienten-Statistiken
\footnotesize
Bemerkungen

* \justifying Eine Likelihood-Quotienten-Statistik setzt die Wahrscheinlichkeitsmassen/dichten 
eines beobachteten Datensatzes $y \in \mathcal{Y}$ unter zwei statistischen 
Modellen *nach Optimierung der jeweiligen Modellparameter* ins Verhältnis.
Ein hoher Wert des Likelihood-Quotienten-Statistik entspricht einer höhereren 
Wahrscheinlichkeitsmasse/dichte des beobachteten Datensatzes $y \in \mathcal{Y}$ 
unter $\mathcal{M}_0$ als unter $\mathcal{M}_1$ und vice versa.

* \justifying  Die Wahrscheinlichkeitsdichten/massen beobachteter Daten nach 
Modellschätzung  unter verschiedenen Modellen zu betrachten ist ein allgemeines 
Vorgehen zum Vergleich von Modellen. Letztlich erlaubt dieses Vorgehen,
verschiedene wissenschaftliche Theorien über die Genese beobachtbarer Daten quantitativ
zu vergleichen und die damit verbundene Unsicherheit zu quantifizieren.

* \justifying  Modellvergleiche sind ein zentrales Thema in der Bayesianischen Inferenz
die die Logik von Likelihood-Quotienten-Statistiken zum Beispiel unter den Begriffen 
der Bayes Factors oder der des Bayesian Information Criterions auf allgemeine 
probabilistische Modelle generalisiert. Allerdings sind, wie hier gesehen, 
Modellvergleiche auch im Rahmen der Frequentistischen Inferenz möglich und sinnvoll, 
Modellvergleiche sind also kein Alleinstellungsmerkmal der Bayesianischen 
gegenüber der Frequentistischen Inferenz.

* Mit dem *reduzierten Modell* und dem *vollständigen Modell*  betrachten
wir im Folgenden zwei spezielle Formen von $\mathcal{M}_0$ und $\mathcal{M}_1$,
respektive, im Kontext des ALMs.

# Likelihood-Quotienten-Statistiken
\footnotesize
\begin{definition}[Vollständiges und reduziertes Modell]
Für $p > 1$ mit $p = p_0 + p_1$ seien
\begin{equation}
X := \begin{pmatrix} X_0 & X_1 \end{pmatrix}  \in \mathbb{R}^{n \times p}
\mbox{ mit }
X_0 \in \mathbb{R}^{n \times p_0}
\mbox{ und }
X_1 \in \mathbb{R}^{n \times p_1},
\end{equation}
sowie
\begin{equation}
\beta := \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} \in \mathbb{R}^p
\mbox{ mit }
\beta_0 \in \mathbb{R}^{p_0}
\mbox{ und }
\beta_1 \in \mathbb{R}^{p_1}
\end{equation}
Partitionierungen einer $n \times p$ Designmatrix und eines $p$-dimensionalen
Betaparametervektors. Dann nennen wir
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
das \textit{vollständige Modell} und
\begin{equation}
\upsilon = X_0\beta_0 + \varepsilon_0 \mbox{ mit } \varepsilon_0 \sim N(0_n,\sigma^2I_n)
\end{equation}
das \textit{reduzierte Modell} und sprechen von einer \textit{Partitionierung eines
(vollständigen) Modells}.
\end{definition}
Bemerkungen

* Man sagt auch, dass das reduzierte Modell im vollständigen Modell \textit{geschachtelt (nested)} ist.

# Likelihood-Quotienten-Statistiken
\footnotesize
\begin{theorem}[Likelihood-Quotient von vollständigem und reduziertem Modell]
\justifying
\normalfont
Für $p = p_0 + p_1, p > 1$ sei eine Partitionierung eines vollständigen ALMs 
gegeben und es seien $\hat{\sigma}^2$ und $\hat{\sigma}^2_0$ die Maximum-Likelihood Schätzer
des Varianzparameters unter vollständigem und reduziertem Modell, respektive.
Weiterhin seien die zwei parametrischen statistischen Modelle $\mathcal{M}_0$ 
und $\mathcal{M}_1$ in der Definition der Likelihood-Quotienten-Statistik durch das 
reduzierte Modell und das vollständige Modell gegeben. 
Dann gilt
\begin{equation}
\Lambda = \left(\frac{\hat{\sigma}^2}{\hat{\sigma}_0^2}\right)^{\frac{n}{2}}
\end{equation}
\end{theorem}

Bemerkungen

* Informell gilt hier
\begin{equation}
\mathcal{M}_0 : \upsilon = X_0\beta_0 + \varepsilon_0 \mbox{ und } \mathcal{M}_1 : \upsilon = X\beta + \varepsilon
\end{equation}

# Likelihood-Quotienten-Statistiken

\footnotesize
\underline{Beweis}

Wir erinnern zunächst daran, dass die Maximum-Likelihood Schätzer des Varianzparameters durch
\begin{equation}
\hat{\sigma}^2 = \frac{1}{n}\left(\upsilon - X\hat{\beta}\right)^T\left(\upsilon - X\hat{\beta}\right)
\mbox{ und }
\hat{\sigma}^2_0 = \frac{1}{n}\left(\upsilon - X_0\hat{\beta}_0\right)^T\left(\upsilon - X_0\hat{\beta}_0\right)
\end{equation}
respektive, gegeben sind, wobei $\hat{\beta}$ und $\hat{\beta}_0$ die Maximum-Likelihood Schätzer
der Betaparameter unter vollständigem und reduziertem Modell, respektive, bezeichnen.  Weiterhin halten wir fest, dass 
für die Likelihood-Funktion des vollständigem Modells an der Stelle der Maximum-Likelihood Schätzer gilt, dass
\begin{align}
\begin{split}
L^y_1(\hat{\beta}, \hat{\sigma}^2)
& = (2\pi)^{-\frac{n}{2}}(\hat{\sigma}^2)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\hat{\sigma}^2}(y - X\hat{\beta})^T(y - X\hat{\beta})\right) \\
& = (2\pi)^{-\frac{n}{2}}(\hat{\sigma}^2)^{-\frac{n}{2}}\exp\left(-\frac{n}{2}\frac{(y - X\hat{\beta})^T(y - X\hat{\beta})}{(y - X\hat{\beta})^T(y - X\hat{\beta})}\right) \\
& = (2\pi)^{-\frac{n}{2}}(\hat{\sigma}^2)^{-\frac{n}{2}}e^{-\frac{n}{2}}
\end{split}
\end{align}
und analog, dass für die Likelihood-Funktion des reduzierten Modells an der Stelle der Maximum-Likelihood Schätzer gilt, dass
\begin{align}
\begin{split}
L^y_0(\hat{\beta}_0, \hat{\sigma}^2_0)
& = (2\pi)^{-\frac{n}{2}}(\hat{\sigma}^2_0)^{-\frac{n}{2}}e^{-\frac{n}{2}}
\end{split}
\end{align}
Damit ergibt sich dann aber
\begin{equation*}
\Lambda 
= \frac{\max_{\theta_0 \in \Theta_0}L^\upsilon_0(\theta_0)}{\max_{\theta_1 \in \Theta_1}L^\upsilon_1(\theta_1)} 
= \frac{L^\upsilon_0(\hat{\beta}_0, \hat{\sigma}_0^2)}{L^\upsilon_1(\hat{\beta}, \hat{\sigma}^2)}
= \frac{(2\pi)^{-\frac{n}{2}}(\hat{\sigma}^2_0)^{-\frac{n}{2}}e^{-\frac{n}{2}}}{(2\pi)^{-\frac{n}{2}}(\hat{\sigma}^2)^{-\frac{n}{2}}e^{-\frac{n}{2}}}
= \left(\frac{\hat{\sigma}^2_0}{\hat{\sigma}^2}\right)^{-\frac{n}{2}}
= \left(\frac{\hat{\sigma}^2}{\hat{\sigma}^2_0}\right)^{\frac{n}{2}}
\end{equation*}

#
\vfill
\large
\setstretch{3}
F-Zufallsvariablen

Likelihood-Quotienten

**Definition und Verteilung**

Selbstkontrollfragen
\vfill

# Definition und Verteilung
\footnotesize
\begin{definition}[F-Statistik]
Für $X \in \mathbb{R}^{n \times p}, \beta \in \mathbb{R}^p$ und $\sigma^2 > 0$
sei ein ALM der Form
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
mit der Partitionierung
\begin{equation}
X      = \begin{pmatrix} X_0     & X_1      \end{pmatrix},
X_0      \in \mathbb{R}^{n\times p_0},
X_1      \in \mathbb{R}^{n\times p_1},
\mbox{ und }
\beta := \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix},
\beta_0 \in \mathbb{R}^{p_0},
\beta_1 \in \mathbb{R}^{p_1},
\end{equation}
mit $p = p_0 + p_1$ gegeben. Weiterhin seien mit
\begin{equation}
\hat{\beta}_0 := (X_0^TX_0)^{-1} X_0^T\upsilon \mbox{ und } \hat{\beta} := (X^TX)^{-1}X^T\upsilon
\end{equation}
die Residuenvektoren
\begin{equation}
\hat{\varepsilon}_0 := \upsilon - X_0\hat{\beta}_0 \mbox{ und } \hat{\varepsilon} := \upsilon - X\hat{\beta}
\end{equation}
definiert. Dann ist die F-Statistik definiert als
\begin{equation}
F := \frac{(\hat{\varepsilon}_0^T\hat{\varepsilon}_0-\hat{\varepsilon}^T\hat{\varepsilon})/p_1}{\hat{\varepsilon}^T\hat{\varepsilon}/(n-p)}
\end{equation}
\end{definition}

# Definition und Verteilung
\footnotesize
Bemerkungen

* \justifying Der Zähler der F-Statistik
\begin{equation}
\frac{\hat{\varepsilon}_0^T\hat{\varepsilon}_0 - \hat{\varepsilon}^T\hat{\varepsilon}}{p_1}
\end{equation}
misst, inwieweit die $p_1$ Regressoren in $X_1$ die Residualquadratsumme reduzieren und zwar
im Verhältnis zur Anzahl dieser Regressoren. Das heißt, dass bei gleicher Größe der
Residualquadratsummenreduktion (und gleichem Nenner) ein größerer $F$ Wert resultiert,
wenn diese durch weniger zusätzliche Regressoren resultiert, also $p_1$ klein ist (und vice versa).
Im Sinne der Anzahl der Spalten von $X$ und der entsprechenden Komponenten von $\beta$
favorisiert die $F$-Statistik also weniger "komplexe" Modelle.
* Für den Nenner der F-Statistik gilt
\begin{equation}
\frac{\hat{\varepsilon}^T\hat{\varepsilon}}{n-p} = \hat{\sigma}^2,
\end{equation}
wobei $\hat{\sigma}^2$ hier der aufgrund des vollständigen Modells geschätzte
Schätzer von $\sigma^2$ ist. Werden die Daten tatsächlich unter dem reduzierten Modell
generiert, so kann das vollständige Modell dies durch $\hat{\beta}_1 \approx 0_{p_1}$
abbilden und erreicht eine ähnliche $\sigma^2$ Schätzung wie das reduzierte Modell.
Werden die Daten de-facto unter dem vollständigem Modell generiert, so ist
$\hat{\varepsilon}^T\hat{\varepsilon}/(n-p)$ ein besserer Schätzer von $\sigma^2$
als $\hat{\varepsilon}^T_0\hat{\varepsilon_0}/(n-p)$, da sich für diesen Datenvariabilität,
die nicht durch die $p_0$ Regressoren in $X_0$ erklärt wird, in der Schätzung von $\sigma^2$
widerspiegeln würde. Der Nenner der F-Statistik ist also in beiden Fällen der sinnvollere
Schätzer von $\sigma^2$.
* Zusammengenommen misst die F-Statistik also die Residualquadratsummenreduktion
durch die $p_1$ Regressoren in $X_1$ gegenüber den  $p_0$ Regressoren in $X_0$
pro Datenvariabilitäts ($\sigma^2$)- und Regressor ($p_1$)-Einheit.


# Definition und Verteilung
\vspace{2mm}
\footnotesize
\begin{theorem}[F-Statistik und Likelihood-Quotienten-Statistik]
\justifying
\normalfont
Es sei die Partitionierung eines ALMs in ein vollständiges und ein reduziertes Modell
gegeben und $F$ und $\Lambda$ seien die entsprechenden F- und Likelihood-Quotienten-Statistiken. 
Dann gilt
\begin{equation}
F = \frac{n-p}{p_1}\left(\Lambda^{-\frac{2}{n}} -1 \right).
\end{equation}
\end{theorem}
\vspace{-2mm}
Bemerkungen

* Zwischen der F- und der Likelihood-Quotienten-Statistik besteht ein funktionaler reziproker Zusammenhang.
* Für $\Lambda = 1$ gilt $F = 0$.
* Wir visualisieren den funktionalen Zusammenhang für $n = 12, p = 2, p_1 = 1$ untenstehend.

```{r, eval = F, echo = F}
# Modellformulierung
n           = 12
p           = 2
p_1         = 1
l_min       = 0.01
l_max       = 1
l_res       = 1e3
Lambda      = seq(l_min, l_max, len = l_res)
Eff         = ((n-p)/p_1)*((Lambda**(-2/n)) -1)

# Visualisierung
graphics.off()
library(latex2exp)
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
Lambda,
Eff,
type        = "l",
lty         = 1,
lwd         = 2,
ylim        = c(0,12),
xlim        = c(l_min,l_max),
ylab        = "F",
xlab        = TeX("$\\Lambda"))
dev.copy2pdf(
file   = file.path("./8_Abbildungen/alm_8_F_Lambda.pdf"),
width  = 5,
height = 4)
```
\vspace{-2mm}
```{r, echo = FALSE, out.width = "50%"}
knitr::include_graphics("8_Abbildungen/alm_8_F_Lambda.pdf")
```

# Definition und Verteilung
\footnotesize
\underline{Beweis}

Wir erinnern zunächst daran, dass die Maximum-Likelihood Schätzer des Varianzparameters durch
\begin{equation}
\hat{\sigma}^2 = \frac{1}{n}\left(\upsilon - X\hat{\beta}\right)^T\left(\upsilon - X\hat{\beta}\right) = \frac{\hat{\varepsilon}^T\hat{\varepsilon}}{n}
\mbox{ und }
\hat{\sigma}^2_0 = \frac{1}{n}\left(\upsilon - X_0\hat{\beta}_0\right)^T\left(\upsilon - X_0\hat{\beta}_0\right) = \frac{\hat{\varepsilon}^T_0\hat{\varepsilon}_0}{n}
\end{equation}
gegeben sind. Mit der Definition der F-Statistik und der Form der Likelihood-Quotienten-Statistik
für den Vergleich von reduziertem und vollständigem Modell ergibt sich dann
\begin{align}
\begin{split}
F 
& = \frac{(\hat{\varepsilon}_0^T\hat{\varepsilon}_0 - \hat{\varepsilon}^T\hat{\varepsilon})/p_1}{\hat{\varepsilon}^T\hat{\varepsilon}/(n-p)} \\
& = \frac{n(\hat{\sigma}^2_0 - \hat{\sigma}^2)/p_1}{n\hat{\sigma}^2/(n-p)} \\
& = \frac{n-p}{p_1} \frac{\hat{\sigma}^2_0 - \hat{\sigma}^2 }{\hat{\sigma}^2} \\
& = \frac{n-p}{p_1} \left(\frac{\hat{\sigma}^2_0}{\hat{\sigma}^2} - \frac{\hat{\sigma}^2}{\hat{\sigma}^2} \right)  \\
& = \frac{n-p}{p_1} \left(\Lambda^{-\frac{2}{n}} - 1\right)
\end{split}
\end{align}

# Definition und Verteilung
\vspace{1mm}
\setstretch{.9}
\small
Beispiel (1)  Einfache lineare Regression
\footnotesize
\begin{equation}
X = \begin{pmatrix} X_0 & X_1 \end{pmatrix}, X_0 := 1_{n}, X_1 := (x_1,...,x_n)^T,
\beta  := \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix},
\beta_0 = \mbox{Interzept,  }
\beta_1 = \mbox{Steigung }
\end{equation}
\tiny
```{r}
# Modellformulierung
library(MASS)                                             # Multivariate Normalverteilung
nmod   = 2                                                # Anzahl Modelle
n      = 10                                               # Anzahl Datenpunkte
p      = 2                                                # Anzahl Betaparameter
p_0    = 1                                                # Anzahl Betaparameter reduziertes Modell
p_1    = 1                                                # Anzahl zusätzlicher Betaparameter vollständiges Modell
p      = p_0 + p_1                                        # Anzahl Betaparameter im vollständigem Modell
x      = 1:n                                              # Prädiktorwerte
X      = matrix(c(rep(1,n),x), nrow = n)                  # Designmatrix des vollständigen Modells
X_0    = X[,1]                                            # Designmatrix des reduzierten Modells
I_n    = diag(n)                                          # n x n Einheitsmatrix
beta   = matrix(c(1,0,1,.5), nrow = 2)                    # wahre , aber unbekannte , Betaparameter
nscn   = ncol(beta)                                       # Anzahl wahrer, aber unbekannter, Hypothesenszenarien
sigsqr = 1                                                # wahrer, aber unbekannter, Varianzparameter

# Modellsimulation und Evaluierung
Eff    = matrix(rep(NaN, nscn), nrow = nscn)              # F-Statistiik Realisierungsarray
for(s in 1:nscn){                                         # Szenarieniterationen
  y               = mvrnorm(1, X %*%beta[,s], sigsqr*I_n) # Datenrealisierung
  beta_hat_0      = solve(t(X_0)%*%X_0)%*%t(X_0)%*%y      # Betaparameterschätzer reduziertes Modell
  beta_hat        = solve(t(X)  %*%X  )%*%t(X)  %*%y      # Betaparameterschätzer vollständiges Modell
  eps_0_hat       = y-X_0%*%beta_hat_0                    # Residuenvektor reduziertes Modell
  eps_hat         = y-X%*%beta_hat                        # Residuenvektor vollständiges Modell
  eps_0_eps_0_hat = t(eps_0_hat) %*% eps_0_hat            # RQS reduziertes Modell
  eps_eps_hat     = t(eps_hat)   %*% eps_hat              # RQS vollständiges Modell
  Eff[s]          = (((eps_0_eps_0_hat-eps_eps_hat)/p_1)/ # F-Statistik
                       (eps_eps_hat/(n-p)))}
```

```{r, echo = F}
cat("F-Statistik für beta_1  = 0_{p_1}:", Eff[1],
    "\nF-Statistik für beta_1 != 0_{p_1}:", Eff[2])
```

# Definition und Verteilung
\footnotesize
\begin{theorem}[F-Statistik]
\justifying
\normalfont
Für $X \in \mathbb{R}^{n \times p}, \beta \in \mathbb{R}^p$ und $\sigma^2 > 0$
sei ein ALM der Form
\begin{equation}
\upsilon = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
mit der Partitionierung
\begin{equation}
X      = \begin{pmatrix} X_0     & X_1      \end{pmatrix},
X_0      \in \mathbb{R}^{n\times p_0},
X_1      \in \mathbb{R}^{n\times p_1},
\mbox{ und }
\beta := \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix},
\beta_0 \in \mathbb{R}^{p_0},
\beta_1 \in \mathbb{R}^{p_1},
\end{equation}
mit $p = p_0 + p_1$ gegeben. Schließlich sei
\begin{equation}
c := \begin{pmatrix} 0_{p_0} \\ 1_{p_1} \end{pmatrix} \in \mathbb{R}^p
\end{equation}
ein Vektor. Dann gilt
\begin{equation}
F \sim f(\delta, p_1, n-p) \mbox{ mit } \delta := \frac{c^T\beta \left(c^T(X^TX)^{-1}c\right)^{-1}c^T \beta}{\sigma^2}
\end{equation}
\end{theorem}

Bemerkungen

* Wir verzichten auf einen Beweis.
* $F$ ist eine Funktion der Parameterschätzer, $\delta$ ist eine Funktion der wahren, aber unbekannten, Parameter.
* Diese Verteilung von $F$ kann zum Nullhypothesentesten und zur Powerfunktionsevaluation genutzt werden.

# Definition und Verteilung
\vspace{2mm}
\setstretch{.9}
\small
Beispiel (1)  Einfache lineare Regression
\footnotesize
\begin{equation}
X = \begin{pmatrix} X_0 & X_1 \end{pmatrix}, X_0 := 1_{n}, X_1 := (x_1,...,x_n)^T,
\beta  := \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix},
\beta_0 = \mbox{Interzept,  }
\beta_1 = \mbox{Steigung }
\end{equation}
\tiny
```{r}
# Modellformulierung
library(MASS)                                               # Multivariate Normalverteilung
nmod   = 2                                                  # Anzahl Modelle
n      = 10                                                 # Anzahl Datenpunkte
p_0    = 1                                                  # Anzahl Betaparameter im reduzierten Modell
p_1    = 1                                                  # Anzahl additiver Betaparameter im vollständigen Modell
p      = p_0 + p_1                                          # Anzahl Betaparameter im vollständigem Modell
x      = 1:n                                                # Prädiktorwerte
X      = matrix(c(rep(1,n),x), nrow = n)                    # Designmatrix des vollständigen Modells
X_0    = X[,1]                                              # Designmatrix des reduzierten Modells
I_n    = diag(n)                                            # n x n Einheitsmatrix
beta   = matrix(c(1,0,1,.5), nrow = 2)                      # wahre , aber unbekannte , Betaparameter
nscn   = ncol(beta)                                         # Anzahl wahrer, aber unbekannter, Hypothesenszenarien
sigsqr = 1                                                  # wahrer, aber unbekannter, Varianzparameter
c      = matrix((c(0,1)), nrow = 2)                         # Vektor  

# Frequentistische Simulation
nsim   = 1e4                                                # Anzahl Realisierungen des n-dimensionalen ZVs
delta  = rep(NaN,nscn)                                      # Nichtzentralitätsparameterarray
Eff    = matrix(rep(NaN, nscn*nsim), nrow = nscn)           # F-Statistiik Realisierungsarray
for(s in 1:nscn){                                           # Szenarieniterationen
  delta[s] = (t(t(c)%*%beta[,s])%*%                         # Nichtzentralitätsparameter
              solve(t(c)%*%solve(t(X)%*%X)%*%c) %*%
              (t(c)%*%beta[,s])/sigsqr)
  for(i in 1:nsim){                                         # Simulationsiterationen
    y               = mvrnorm(1, X %*%beta[,s], sigsqr*I_n) # Datenrealisierung
    beta_hat_0      = solve(t(X_0)%*%X_0)%*%t(X_0)%*%y      # Betaparameterschätzer reduziertes Modell
    beta_hat        = solve(t(X)  %*%X  )%*%t(X)  %*%y      # Betaparameterschätzer vollständiges Modell
    eps_0_hat       = y-X_0%*%beta_hat_0                    # Residuenvektor reduziertes Modell
    eps_hat         = y-X%*%beta_hat                        # Residuenvektor vollständiges Modell
    eps_0_eps_0_hat = t(eps_0_hat) %*% eps_0_hat            # RQS reduziertes Modell
    eps_eps_hat     = t(eps_hat)   %*% eps_hat              # RQS vollständiges Modell
    Eff[s,i]        = (((eps_0_eps_0_hat-eps_eps_hat)/p_1)/ # F-Statistik
                         (eps_eps_hat/(n-p)))}}
```

# Definition und Verteilung
\vspace{1mm}
Beispiel (1)  Einfache lineare Regression
\vspace{2mm}

```{r, eval = F, echo = F}

# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# F-Statistik Ergebnisraum
lab         = c(TeX("$\\beta = (1,0)^T$"), TeX("$\\beta = (1,0.5)^T$") )
xlimits     = list(c(.025,5), c(0,70))
ylimits     = list(c(0,2.5), c(0,0.05))
breaks      = c(600,200)

# F-Teststatistiken
for(s in 1:nscn){
  xlims  = xlimits[[s]]
  f_min  = xlims[1]
  f_max  = xlims[2]
  f_res  = 1e3
  f      = seq(f_min, f_max, len = f_res)
  p_f    = df(f,p_1,n-p, delta[s])
  hist(
  Eff[s,],
  breaks = breaks[s],
  col    = "gray90",
  prob   = TRUE,
  xlab   = TeX("$F$"),
  ylab   = "",
  xlim   = xlims,
  ylim   = ylimits[[s]],
  main   = lab[s])
  lines(
  f,
  p_f,
  type  = "l",
  lwd   = 2,
  col   = "darkorange")
}

# Speichern
dev.copy2pdf(
file        = file.path("./8_Abbildungen/alm_8_F_Statistik.pdf"),
width       = 8,
height      = 4)
```
\vfill
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("8_Abbildungen/alm_8_F_Statistik.pdf")
```
\vfill


# Definition und Verteilung
\small
Ausblick

* \justifying Die Theorie von T- und F-Statistiken wird unter dem Begriff der 
*Allgemeinen Linearen Hypothese* verallgemeinert und integriert. Dabei betrachtet
allgemeine lineare Funktionen der Betaparameter der Form $C^T\beta$, wobei 
$C \in \mathbb{R}^{p \times k}$ eine beliebige Matrix ist als Grundlage von Hypothesen. 
Zum Beispiel ergibt sich für $C \in \mathbb{R}^{p \times 1}$ die hier und in 
Einheit (7) T-Statistiken  betrachten Kontrastgewichtsvektoren. Im Kontext der 
*Allgemeinen Linearen Hypothese* kann man weiterhin zeigen, dass $F = T^2$, dass 
also auch das Quadrat einer T-Statistik $f$-verteilt ist und T-Statistiken damit 
(nur) spezielle F-Statistiken sind. \vspace{2mm}

* \justifying Dennoch wird in der Anwendung sehr stark zwischen T- und F-Statistiken unterschieden
und es ist sinnvoll, sich der unterschiedlichen Anwendungsfälle von T- und F-Statistiken
bewusst zu sein. Gute Einführungen in die Theorie der Allgemeinen Linearen Hypothese bieten z.B.
@searle_1971, Kapitel 3, @rencher_2008, Kapitel 8 oder @christensen_2011, Kapitel 3. 

# 
\vfill
\large
\setstretch{3}
F-Zufallsvariablen

Likelihood-Quotienten-Statistiken

Definition und Verteilung

**Selbstkontrollfragen**
\vfill

# Selbstkontrollfragen
\footnotesize
\setstretch{2.5}

1. Skizzieren Sie die $f$-Verteilung für $\nu_1 = 2, \nu_2 = 13$ und $\nu_1 = 4, \nu_2 = 13$.
1. Geben Sie die Definition der Likelihood-Quotienten-Statistik wieder.
1. Erläutern Sie die Definition der Likelihood-Quotienten-Statistik.
1. Geben Sie die Definition eines vollständigem und eines reduziertem ALMs wieder.
1. Geben Sie das Theorem zum Likelihood-Quotienten von vollständigem und reduzierten ALM wieder.
1. Definieren Sie die F-Statistik.
1. Erläutern Sie den Zähler der F-Statistik.
1. Erläutern Sie den Nenner der F-Statistik.
1. Erläutern Sie die F-Statistik.
1. Geben Sie das Theorem zum Zusammenhang von F-Statistik und Likelihood-Quotienten-Statistik wieder.

# Referenzen
\footnotesize
